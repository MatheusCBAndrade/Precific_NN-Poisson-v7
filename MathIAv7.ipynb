{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando os arquivos txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho para a pasta que contém os arquivos .xlsx\n",
    "path = r'C:\\Users\\mathe\\OneDrive\\Área de Trabalho\\SoccerIA\\Datasets finais europeus xlsx'\n",
    "\n",
    "# Lista todos os arquivos na pasta\n",
    "files = os.listdir(path)\n",
    "\n",
    "# Filtra a lista de arquivos para incluir apenas os arquivos .xlsx\n",
    "xlsx_files = [f for f in files if f.endswith('.xlsx')]\n",
    "\n",
    "colunas_interesse = [\n",
    "    'team1_goals', 'team2_goals',\n",
    "    'team1_total_shots', 'team2_total_shots', 'team1_shots_on_target',\n",
    "    'team2_shots_on_target', 'team1_fouls', 'team2_fouls', 'team1_corners',\n",
    "    'team2_corners', 'team1_yellow_cards', 'team2_yellow_cards',\n",
    "    'team1_red_cards', 'team2_red_cards', 'team1_shots_out',\n",
    "    'team2_shots_out'\n",
    "]\n",
    "\n",
    "# Carrega cada arquivo .xlsx em um dataframe e armazena na lista dfss\n",
    "for file in xlsx_files:\n",
    "    data_path = os.path.join(path, file)\n",
    "    df = pd.read_excel(data_path)\n",
    "\n",
    "    # Calcula a média para as colunas de interesse\n",
    "    medias = df[colunas_interesse].mean()\n",
    "\n",
    "    # Cria o dicionário\n",
    "    dict_medias = medias.to_dict()\n",
    "\n",
    "    # Salva o dicionário em um arquivo .txt\n",
    "    nome_txt = os.path.splitext(file)[0] + '.txt'  # Substitui a extensão por .txt\n",
    "    caminho_txt = os.path.join(path, nome_txt)\n",
    "    \n",
    "    with open(caminho_txt, 'w') as arquivo_txt:\n",
    "        for key, value in dict_medias.items():\n",
    "            linha = f\"{key}: {value}\\n\"\n",
    "            arquivo_txt.write(linha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando os arquivos excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alemanha A.xlsx\n",
      "(4896, 21)\n",
      "Alemanha B.xlsx\n",
      "(1530, 21)\n",
      "Belgica A.xlsx\n",
      "(839, 21)\n",
      "Escocia A.xlsx\n",
      "(4065, 21)\n",
      "Espanha A.xlsx\n",
      "(6460, 21)\n",
      "Espanha B.xlsx\n",
      "(2289, 21)\n",
      "França A.xlsx\n",
      "(5596, 21)\n",
      "França B.xlsx\n",
      "(1039, 21)\n",
      "Grecia A.xlsx\n",
      "(720, 21)\n",
      "Holanda A.xlsx\n",
      "(1456, 21)\n",
      "Inglaterra A.xlsx\n",
      "(7418, 21)\n",
      "Inglaterra B.xlsx\n",
      "(9775, 21)\n",
      "Inglaterra C.xlsx\n",
      "(9608, 21)\n",
      "Inglaterra D.xlsx\n",
      "(9710, 21)\n",
      "Italia B.xlsx\n",
      "(1841, 21)\n",
      "Italia_A.xlsx\n",
      "(6680, 21)\n",
      "Portugal A.xlsx\n",
      "(1530, 21)\n",
      "Turquia A.xlsx\n",
      "(1610, 21)\n",
      "(77062, 21)\n",
      "Index(['championship', 'date', 'team1', 'team2', 'team1_goals', 'team2_goals',\n",
      "       'team1_total_shots', 'team2_total_shots', 'team1_shots_on_target',\n",
      "       'team2_shots_on_target', 'team1_fouls', 'team2_fouls', 'team1_corners',\n",
      "       'team2_corners', 'team1_yellow_cards', 'team2_yellow_cards',\n",
      "       'team1_red_cards', 'team2_red_cards', 'team1_shots_out',\n",
      "       'team2_shots_out', 'season'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho para a pasta que contém os arquivos .xlsx\n",
    "path = r'C:\\Users\\mathe\\OneDrive\\Área de Trabalho\\SoccerIA\\Datasets finais europeus xlsx'\n",
    "\n",
    "# Lista todos os arquivos na pasta\n",
    "files = os.listdir(path)\n",
    "\n",
    "# Filtra a lista de arquivos para incluir apenas os arquivos .xlsx\n",
    "xlsx_files = [f for f in files if f.endswith('.xlsx')]\n",
    "\n",
    "dfss = []\n",
    "\n",
    "colunas_interesse = [\n",
    "    'team1_goals', 'team2_goals',\n",
    "    'team1_total_shots', 'team2_total_shots', 'team1_shots_on_target',\n",
    "    'team2_shots_on_target', 'team1_fouls', 'team2_fouls', 'team1_corners',\n",
    "    'team2_corners', 'team1_yellow_cards', 'team2_yellow_cards',\n",
    "    'team1_red_cards', 'team2_red_cards', 'team1_shots_out',\n",
    "    'team2_shots_out'\n",
    "]\n",
    "\n",
    "# Carrega cada arquivo .xlsx em um dataframe e armazena na lista dfss\n",
    "for file in xlsx_files:\n",
    "    data_path = os.path.join(path, file)\n",
    "    df = pd.read_excel(data_path)\n",
    "    \n",
    "    dfss.append(df)\n",
    "    print(file)\n",
    "    print(df.shape)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "concatenado_df = pd.concat(dfss, ignore_index=True)\n",
    "print(concatenado_df.shape)\n",
    "print(concatenado_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D1' 'D2' 'B1' 'SC0' 'SP1' 'SP2' 'F1' 'F2' 'G1' 'N1' 'E0' 'E1' 'E2' 'E3'\n",
      " 'I2' 'I1' 'P1' 'T1']\n"
     ]
    }
   ],
   "source": [
    "champ_uniques = concatenado_df['championship'].unique()\n",
    "print(champ_uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "(77062, 21)\n"
     ]
    }
   ],
   "source": [
    "nan_counts = concatenado_df.isna().sum()\n",
    "print(list(nan_counts))\n",
    "print(concatenado_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\738720059.py:251: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c.at[i, 'median_conc_corners_lasts5_1_home'] = team1_home['team2_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team1_home['team2_corners'].isna().any() else np.nan\n",
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\738720059.py:252: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c.at[i, 'std_conc_corners_lasts5_1_home'] = team1_home['team2_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team1_home['team2_corners'].isna().any() else np.nan\n",
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\738720059.py:347: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c.at[i, 'median_total_shots_lasts5_2_away'] = team2_away['team2_total_shots'].rolling(5, min_periods=2).median().iloc[-1] if not team2_away['team2_total_shots'].isna().any() else np.nan\n",
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\738720059.py:348: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c.at[i, 'std_total_shots_lasts5_2_away'] = team2_away['team2_total_shots'].rolling(5, min_periods=2).std().iloc[-1] if not team2_away['team2_total_shots'].isna().any() else np.nan\n",
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\738720059.py:351: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c.at[i, 'median_corners_lasts5_2_away'] = team2_away['team2_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team2_away['team2_corners'].isna().any() else np.nan\n",
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\738720059.py:352: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c.at[i, 'std_corners_lasts5_2_away'] = team2_away['team2_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team2_away['team2_corners'].isna().any() else np.nan\n",
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\738720059.py:355: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c.at[i, 'median_conc_corners_lasts5_2_away'] = team2_away['team1_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team2_away['team1_corners'].isna().any() else np.nan\n",
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\738720059.py:356: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c.at[i, 'std_conc_corners_lasts5_2_away'] = team2_away['team1_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team2_away['team1_corners'].isna().any() else np.nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77062, 153)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "combined_df_13c = concatenado_df.copy()\n",
    "\n",
    "combined_df_13c.sort_values('date', inplace=True)\n",
    "\n",
    "\n",
    "combined_df_13c['team1_goals'] = pd.to_numeric(combined_df_13c['team1_goals'], errors='coerce')\n",
    "combined_df_13c['team2_goals'] = pd.to_numeric(combined_df_13c['team2_goals'], errors='coerce')\n",
    "\n",
    "# calculate goal differences\n",
    "combined_df_13c['goal_diff_team1'] = combined_df_13c['team1_goals'] - combined_df_13c['team2_goals']\n",
    "combined_df_13c['goal_diff_team2'] = combined_df_13c['team2_goals'] - combined_df_13c['team1_goals']\n",
    "\n",
    "# calculate corners differences\n",
    "combined_df_13c['corners_diff_team1'] = combined_df_13c['team1_corners'] - combined_df_13c['team2_corners']############# NEW\n",
    "combined_df_13c['corners_diff_team2'] = combined_df_13c['team2_corners'] - combined_df_13c['team1_corners']############# NEW\n",
    "\n",
    "# calculate big wins and losses\n",
    "combined_df_13c['team1_big_win'] = np.where(combined_df_13c['goal_diff_team1'] >= 2, 1, 0)\n",
    "combined_df_13c['team1_big_loss'] = np.where(combined_df_13c['goal_diff_team1'] <= -2, 1, 0)\n",
    "combined_df_13c['team2_big_win'] = np.where(combined_df_13c['goal_diff_team2'] >= 2, 1, 0)\n",
    "combined_df_13c['team2_big_loss'] = np.where(combined_df_13c['goal_diff_team2'] <= -2, 1, 0)\n",
    "\n",
    "# calculate AH-2.5 win and losses\n",
    "combined_df_13c['team1_ah-2.5_win'] = np.where(combined_df_13c['corners_diff_team1'] >= 3, 1, 0)############# NEW\n",
    "combined_df_13c['team1_ah-2.5_loss'] = np.where(combined_df_13c['corners_diff_team1'] <= 2, 1, 0)############# NEW\n",
    "combined_df_13c['team2_ah-2.5_win'] = np.where(combined_df_13c['corners_diff_team2'] >= 3, 1, 0)############# NEW\n",
    "combined_df_13c['team2_ah-2.5_loss'] = np.where(combined_df_13c['corners_diff_team2'] <= 2, 1, 0)############# NEW\n",
    "\n",
    "\n",
    "# calculate AH+2.5 win and losses\n",
    "combined_df_13c['team1_ah+2.5_win'] = np.where(combined_df_13c['corners_diff_team1'] >= -2, 1, 0)############# NEW\n",
    "combined_df_13c['team1_ah+2.5_loss'] = np.where(combined_df_13c['corners_diff_team1'] <= -3, 1, 0)############# NEW\n",
    "combined_df_13c['team2_ah+2.5_win'] = np.where(combined_df_13c['corners_diff_team2'] >= -2, 1, 0)############# NEW\n",
    "combined_df_13c['team2_ah+2.5_loss'] = np.where(combined_df_13c['corners_diff_team2'] <= -3, 1, 0)############# NEW\n",
    "\n",
    "\n",
    "# calculate over4.5 win and losses\n",
    "combined_df_13c['team1_over4.5'] = np.where(combined_df_13c['team1_corners'] >= 5, 1, 0)############# NEW\n",
    "combined_df_13c['team1_under4.5'] = np.where(combined_df_13c['team1_corners'] <= 4, 1, 0)############# NEW\n",
    "combined_df_13c['team2_over4.5'] = np.where(combined_df_13c['team2_corners'] >= 5, 1, 0)############# NEW\n",
    "combined_df_13c['team2_under4.5'] = np.where(combined_df_13c['team2_corners'] <= 4, 1, 0)############# NEW\n",
    "\n",
    "# calculate over3.5 win and losses\n",
    "combined_df_13c['team1_over3.5'] = np.where(combined_df_13c['team1_corners'] >= 4, 1, 0)############# NEW\n",
    "combined_df_13c['team1_under3.5'] = np.where(combined_df_13c['team1_corners'] <= 3, 1, 0)############# NEW\n",
    "combined_df_13c['team2_over3.5'] = np.where(combined_df_13c['team2_corners'] >= 4, 1, 0)############# NEW\n",
    "combined_df_13c['team2_under3.5'] = np.where(combined_df_13c['team2_corners'] <= 3, 1, 0)############# NEW\n",
    "\n",
    "# calculate over6.5 win and losses\n",
    "combined_df_13c['team1_over6.5'] = np.where(combined_df_13c['team1_corners'] >= 7, 1, 0)############# NEW\n",
    "combined_df_13c['team1_under6.5'] = np.where(combined_df_13c['team1_corners'] <= 6, 1, 0)############# NEW\n",
    "combined_df_13c['team2_over6.5'] = np.where(combined_df_13c['team2_corners'] >= 7, 1, 0)############# NEW\n",
    "combined_df_13c['team2_under6.5'] = np.where(combined_df_13c['team2_corners'] <= 6, 1, 0)############# NEW\n",
    "\n",
    "\n",
    "# Initialize these columns with 0\n",
    "combined_df_13c['team1_big_wins_last5'] = 0\n",
    "combined_df_13c['team1_big_losses_last5'] = 0\n",
    "combined_df_13c['team2_big_wins_last5'] = 0\n",
    "combined_df_13c['team2_big_losses_last5'] = 0\n",
    "\n",
    "combined_df_13c['team1_ah-2.5_wins_last5'] = 0############# NEW\n",
    "combined_df_13c['team1_ah-2.5_losses_last5'] = 0############# NEW\n",
    "combined_df_13c['team2_ah-2.5_wins_last5'] = 0############# NEW\n",
    "combined_df_13c['team2_ah-2.5_losses_last5'] = 0############# NEW\n",
    "\n",
    "combined_df_13c['team1_ah+2.5_wins_last5'] = 0############# NEW\n",
    "combined_df_13c['team1_ah+2.5_losses_last5'] = 0############# NEW\n",
    "combined_df_13c['team2_ah+2.5_wins_last5'] = 0############# NEW\n",
    "combined_df_13c['team2_ah+2.5_losses_last5'] = 0############# NEW\n",
    "\n",
    "combined_df_13c['team1_over3.5_last5'] = 0############# NEW\n",
    "combined_df_13c['team1_under3.5_last5'] = 0############# NEW\n",
    "combined_df_13c['team2_over3.5_last5'] = 0############# NEW\n",
    "combined_df_13c['team2_under3.5_last5'] = 0############# NEW\n",
    "\n",
    "combined_df_13c['team1_over4.5_last5'] = 0############# NEW\n",
    "combined_df_13c['team1_under4.5_last5'] = 0############# NEW\n",
    "combined_df_13c['team2_over4.5_last5'] = 0############# NEW\n",
    "combined_df_13c['team2_under4.5_last5'] = 0############# NEW\n",
    "\n",
    "combined_df_13c['team1_over6.5_last5'] = 0############# NEW\n",
    "combined_df_13c['team1_under6.5_last5'] = 0############# NEW\n",
    "combined_df_13c['team2_over6.5_last5'] = 0############# NEW\n",
    "combined_df_13c['team2_under6.5_last5'] = 0############# NEW\n",
    "\n",
    "\n",
    "new_cols = ['avg_scr_lasts3_1_home', 'avg_scr_lasts5_1_home', 'avg_scr_lasts3_1_away',\n",
    "            'avg_scr_lasts5_1_away', 'avg_conc_lasts3_1_home', 'avg_conc_lasts5_1_home',\n",
    "            'avg_conc_lasts3_1_away', 'avg_conc_lasts5_1_away', 'avg_scr_lasts3_2_home',\n",
    "            'avg_scr_lasts5_2_home', 'avg_scr_lasts3_2_away', 'avg_scr_lasts5_2_away',\n",
    "            'avg_conc_lasts3_2_home', 'avg_conc_lasts5_2_home', 'avg_conc_lasts3_2_away',\n",
    "            'avg_conc_lasts5_2_away','team1_big_wins_last5', 'team1_big_losses_last5', \n",
    "            'team2_big_wins_last5', 'team2_big_losses_last5',\n",
    "            #abaixo vai ser baseado em finalizações\n",
    "            'avg_total_shots_lasts5_1_home','avg_total_shots_lasts5_1_away','avg_total_shots_lasts5_2_home',\n",
    "            'avg_total_shots_lasts5_2_away', 'avg_otarget_shots_lasts5_1_home','avg_otarget_shots_lasts5_1_away',\n",
    "            'avg_otarget_shots_lasts5_2_home','avg_otarget_shots_lasts5_2_away','avg_out_shots_lasts5_1_home',\n",
    "            'avg_out_shots_lasts5_1_away','avg_out_shots_lasts5_2_home','avg_out_shots_lasts5_2_away',\n",
    "            'avg_conc_total_shots_lasts5_1_home','avg_conc_total_shots_lasts5_1_away',\n",
    "            'avg_conc_total_shots_lasts5_2_home','avg_conc_total_shots_lasts5_2_away',\n",
    "            #abaixo vai ser baseado em corners\n",
    "            'avg_corners_lasts5_1_home','avg_corners_lasts5_1_away', \n",
    "            'avg_corners_conc_lasts5_1_home','avg_corners_conc_lasts5_1_away',\n",
    "            'avg_corners_lasts5_2_home','avg_corners_lasts5_2_away', \n",
    "            'avg_corners_conc_lasts5_2_home', 'avg_corners_conc_lasts5_2_away',\n",
    "            #abaixo vai ser baseado em fouls\n",
    "            'avg_fouls_lasts5_1_home','avg_fouls_lasts5_1_away', \n",
    "            'avg_fouls_conc_lasts5_1_home', 'avg_fouls_conc_lasts5_1_away',\n",
    "            'avg_fouls_lasts5_2_home','avg_fouls_lasts5_2_away', \n",
    "            'avg_fouls_conc_lasts5_2_home', 'avg_fouls_conc_lasts5_2_away',\n",
    "            #novas colunas da v7\n",
    "            'team1_ah-2.5_wins_last5', 'team1_ah-2.5_losses_last5','team2_ah-2.5_wins_last5','team2_ah-2.5_losses_last5',\n",
    "            'team1_ah+2.5_wins_last5','team1_ah+2.5_losses_last5','team2_ah+2.5_wins_last5','team2_ah+2.5_losses_last5',\n",
    "            'team1_over3.5_last5','team1_under3.5_last5','team2_over3.5_last5','team2_under3.5_last5',\n",
    "            'team1_over4.5_last5','team1_under4.5_last5','team2_over4.5_last5','team2_under4.5_last5',\n",
    "            'team1_over6.5_last5','team1_under6.5_last5','team2_over6.5_last5','team2_under6.5_last5',\n",
    "            #v8\n",
    "            'avg_corners_diff_lasts5_1_home', 'median_corners_diff_lasts5_1_home', 'avg_corners_diff_lasts5_1_away',\n",
    "             'median_corners_diff_lasts5_1_away', 'avg_corners_diff_lasts5_2_home','median_corners_diff_lasts5_2_home', \n",
    "             'avg_corners_diff_lasts5_2_away','median_corners_diff_lasts5_2_away'\n",
    "            ]\n",
    "\n",
    "# Create a dictionary with keys as column names and values as np.nan\n",
    "new_cols_dict = {col: np.nan for col in new_cols}\n",
    "\n",
    "# Add new columns to the DataFrame\n",
    "combined_df_13c = combined_df_13c.assign(**new_cols_dict)\n",
    "\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for i, row in combined_df_13c.iterrows():\n",
    "    # For each team, get their past home and away matches before the current date\n",
    "    team1_matches = combined_df_13c[((combined_df_13c['team1'] == row['team1']) | (combined_df_13c['team2'] == row['team1'])) & (combined_df_13c['date'] < row['date']) & (combined_df_13c['season'] == row['season'])].sort_values(by='date')\n",
    "    team2_matches = combined_df_13c[((combined_df_13c['team1'] == row['team2']) | (combined_df_13c['team2'] == row['team2'])) & (combined_df_13c['date'] < row['date']) & (combined_df_13c['season'] == row['season'])].sort_values(by='date')\n",
    "\n",
    "    # For each team, calculate stats for last 5 matches\n",
    "    if not team1_matches.empty:\n",
    "        team1_matches['big_win'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_big_win'], team1_matches['team2_big_win'])\n",
    "        team1_matches['big_loss'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_big_loss'], team1_matches['team2_big_loss'])\n",
    "        combined_df_13c.at[i, 'team1_big_wins_last5'] = team1_matches['big_win'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team1_big_losses_last5'] = team1_matches['big_loss'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para ah-2.5 para a equipe 1\n",
    "        team1_matches['ah-2.5_win'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_ah-2.5_win'], team1_matches['team2_ah-2.5_win'])\n",
    "        team1_matches['ah-2.5_loss'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_ah-2.5_loss'], team1_matches['team2_ah-2.5_loss'])\n",
    "        combined_df_13c.at[i, 'team1_ah-2.5_wins_last5'] = team1_matches['ah-2.5_win'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team1_ah-2.5_losses_last5'] = team1_matches['ah-2.5_loss'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para ah+2.5 para a equipe 1\n",
    "        team1_matches['ah+2.5_win'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_ah+2.5_win'], team1_matches['team2_ah+2.5_win'])\n",
    "        team1_matches['ah+2.5_loss'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_ah+2.5_loss'], team1_matches['team2_ah+2.5_loss'])\n",
    "        combined_df_13c.at[i, 'team1_ah+2.5_wins_last5'] = team1_matches['ah+2.5_win'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team1_ah+2.5_losses_last5'] = team1_matches['ah+2.5_loss'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over3.5 para a equipe 1\n",
    "        team1_matches['over3.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_over3.5'], team1_matches['team2_over3.5'])\n",
    "        team1_matches['under3.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_under3.5'], team1_matches['team2_under3.5'])\n",
    "        combined_df_13c.at[i, 'team1_over3.5_last5'] = team1_matches['over3.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team1_under3.5_last5'] = team1_matches['under3.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over4.5 para a equipe 1\n",
    "        team1_matches['over4.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_over4.5'], team1_matches['team2_over4.5'])\n",
    "        team1_matches['under4.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_under4.5'], team1_matches['team2_under4.5'])\n",
    "        combined_df_13c.at[i, 'team1_over4.5_last5'] = team1_matches['over4.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team1_under4.5_last5'] = team1_matches['under4.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over6.5 para a equipe 1\n",
    "        team1_matches['over6.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_over6.5'], team1_matches['team2_over6.5'])\n",
    "        team1_matches['under6.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_under6.5'], team1_matches['team2_under6.5'])\n",
    "        combined_df_13c.at[i, 'team1_over6.5_last5'] = team1_matches['over6.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team1_under6.5_last5'] = team1_matches['under6.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "\n",
    "\n",
    "    if not team2_matches.empty:\n",
    "        team2_matches['big_win'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_big_win'], team2_matches['team2_big_win'])\n",
    "        team2_matches['big_loss'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_big_loss'], team2_matches['team2_big_loss'])\n",
    "        combined_df_13c.at[i, 'team2_big_wins_last5'] = team2_matches['big_win'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team2_big_losses_last5'] = team2_matches['big_loss'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para ah-2.5 para a equipe 2\n",
    "        team2_matches['ah-2.5_win'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_ah-2.5_win'], team2_matches['team2_ah-2.5_win'])\n",
    "        team2_matches['ah-2.5_loss'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_ah-2.5_loss'], team2_matches['team2_ah-2.5_loss'])\n",
    "        combined_df_13c.at[i, 'team2_ah-2.5_wins_last5'] = team2_matches['ah-2.5_win'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team2_ah-2.5_losses_last5'] = team2_matches['ah-2.5_loss'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para ah+2.5 para a equipe 2\n",
    "        team2_matches['ah+2.5_win'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_ah+2.5_win'], team2_matches['team2_ah+2.5_win'])\n",
    "        team2_matches['ah+2.5_loss'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_ah+2.5_loss'], team2_matches['team2_ah+2.5_loss'])\n",
    "        combined_df_13c.at[i, 'team2_ah+2.5_wins_last5'] = team2_matches['ah+2.5_win'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team2_ah+2.5_losses_last5'] = team2_matches['ah+2.5_loss'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over3.5  para a equipe 2\n",
    "        team2_matches['over3.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_over3.5'], team2_matches['team2_over3.5'])\n",
    "        team2_matches['under3.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_under3.5'], team2_matches['team2_under3.5'])\n",
    "        combined_df_13c.at[i, 'team2_over3.5_last5'] = team2_matches['over3.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team2_under3.5_last5'] = team2_matches['under3.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over4.5  para a equipe 2\n",
    "        team2_matches['over4.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_over4.5'], team2_matches['team2_over4.5'])\n",
    "        team2_matches['under4.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_under4.5'], team2_matches['team2_under4.5'])\n",
    "        combined_df_13c.at[i, 'team2_over4.5_last5'] = team2_matches['over4.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team2_under4.5_last5'] = team2_matches['under4.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over6.5  para a equipe 2\n",
    "        team2_matches['over6.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_over6.5'], team2_matches['team2_over6.5'])\n",
    "        team2_matches['under6.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_under6.5'], team2_matches['team2_under6.5'])\n",
    "        combined_df_13c.at[i, 'team2_over6.5_last5'] = team2_matches['over6.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_13c.at[i, 'team2_under6.5_last5'] = team2_matches['under6.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for i, row in combined_df_13c.iterrows():\n",
    "    team1_home = combined_df_13c[(combined_df_13c['date'] < row['date']) & (combined_df_13c['team1'] == row['team1']) & (combined_df_13c['season'] == row['season'])]\n",
    "    team1_away = combined_df_13c[(combined_df_13c['date'] < row['date']) & (combined_df_13c['team2'] == row['team1']) & (combined_df_13c['season'] == row['season'])]\n",
    "    \n",
    "    team2_home = combined_df_13c[(combined_df_13c['date'] < row['date']) & (combined_df_13c['team1'] == row['team2']) & (combined_df_13c['season'] == row['season'])]\n",
    "    team2_away = combined_df_13c[(combined_df_13c['date'] < row['date']) & (combined_df_13c['team2'] == row['team2']) & (combined_df_13c['season'] == row['season'])]\n",
    "\n",
    "    if not team1_home.empty:\n",
    "        combined_df_13c.at[i, 'avg_scr_lasts3_1_home'] = team1_home['team1_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team1_home['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_scr_lasts5_1_home'] = team1_home['team1_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_lasts3_1_home'] = team1_home['team2_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team1_home['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_lasts5_1_home'] = team1_home['team2_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team2_goals'].isna().any() else np.nan\n",
    "        ####################################### ABAIXO É SOBRE FINALIZAÇÕES ###########################################\n",
    "        combined_df_13c.at[i, 'avg_total_shots_lasts5_1_home'] = team1_home['team1_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_total_shots'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_total_shots_lasts5_1_home'] = team1_home['team2_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team2_total_shots'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_otarget_shots_lasts5_1_home'] = team1_home['team1_shots_on_target'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_shots_on_target'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_out_shots_lasts5_1_home'] = team1_home['team1_shots_out'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_shots_out'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE CORNERS ##########################################\n",
    "        combined_df_13c.at[i, 'avg_corners_lasts5_1_home'] = team1_home['team1_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_corners_conc_lasts5_1_home'] = team1_home['team2_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team2_corners'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE FOULS ##########################################\n",
    "        combined_df_13c.at[i, 'avg_fouls_lasts5_1_home'] = team1_home['team1_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_fouls'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_fouls_conc_lasts5_1_home'] = team1_home['team2_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team2_fouls'].isna().any() else np.nan\n",
    "\n",
    "        # Mediana e Desvio Padrão para Finalizações (Shots)\n",
    "        combined_df_13c.at[i, 'median_total_shots_lasts5_1_home'] = team1_home['team1_total_shots'].rolling(5, min_periods=2).median().iloc[-1] if not team1_home['team1_total_shots'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_total_shots_lasts5_1_home'] = team1_home['team1_total_shots'].rolling(5, min_periods=2).std().iloc[-1] if not team1_home['team1_total_shots'].isna().any() else np.nan\n",
    "        \n",
    "        # Mediana e Desvio Padrão para Corners\n",
    "        combined_df_13c.at[i, 'median_corners_lasts5_1_home'] = team1_home['team1_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team1_home['team1_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_corners_lasts5_1_home'] = team1_home['team1_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team1_home['team1_corners'].isna().any() else np.nan\n",
    "\n",
    "        # Mediana e Desvio Padrão para Corners Concedidos\n",
    "        combined_df_13c.at[i, 'median_conc_corners_lasts5_1_home'] = team1_home['team2_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team1_home['team2_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_conc_corners_lasts5_1_home'] = team1_home['team2_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team1_home['team2_corners'].isna().any() else np.nan\n",
    "        \n",
    "        \n",
    "        # Média e Mediana do saldo da diferença de escanteios para o time 1 nas últimas 5 partidas em casa\n",
    "        combined_df_13c.at[i, 'avg_corners_diff_lasts5_1_home'] = team1_home['corners_diff_team1'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['corners_diff_team1'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'median_corners_diff_lasts5_1_home'] = team1_home['corners_diff_team1'].rolling(5, min_periods=2).median().iloc[-1] if not team1_home['corners_diff_team1'].isna().any() else np.nan\n",
    "\n",
    "\n",
    "    if not team1_away.empty:\n",
    "        combined_df_13c.at[i, 'avg_scr_lasts3_1_away'] = team1_away['team2_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team1_away['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_scr_lasts5_1_away'] = team1_away['team2_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_lasts3_1_away'] = team1_away['team1_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team1_away['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_lasts5_1_away'] = team1_away['team1_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team1_goals'].isna().any() else np.nan\n",
    "        ####################################### ABAIXO É SOBRE FINALIZAÇÕES ###########################################\n",
    "        combined_df_13c.at[i, 'avg_total_shots_lasts5_1_away'] = team1_away['team2_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_total_shots'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_otarget_shots_lasts5_1_away'] = team1_away['team2_shots_on_target'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_shots_on_target'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_out_shots_lasts5_1_away'] = team1_away['team2_shots_out'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_shots_out'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_total_shots_lasts5_1_away'] = team1_away['team1_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team1_total_shots'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE CORNERS ##########################################\n",
    "        combined_df_13c.at[i, 'avg_corners_lasts5_1_away'] = team1_away['team2_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_corners_conc_lasts5_1_away'] = team1_away['team1_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team1_corners'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE FOULS ##########################################\n",
    "        combined_df_13c.at[i, 'avg_fouls_lasts5_1_away'] = team1_away['team2_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_fouls'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_fouls_conc_lasts5_1_away'] = team1_away['team1_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team1_fouls'].isna().any() else np.nan\n",
    "\n",
    "        # Mediana e Desvio Padrão para Finalizações (Shots)\n",
    "        combined_df_13c.at[i, 'median_total_shots_lasts5_1_away'] = team1_away['team2_total_shots'].rolling(5, min_periods=2).median().iloc[-1] if not team1_away['team2_total_shots'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_total_shots_lasts5_1_away'] = team1_away['team2_total_shots'].rolling(5, min_periods=2).std().iloc[-1] if not team1_away['team2_total_shots'].isna().any() else np.nan\n",
    "        \n",
    "        # Mediana e Desvio Padrão para Corners\n",
    "        combined_df_13c.at[i, 'median_corners_lasts5_1_away'] = team1_away['team2_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team1_away['team2_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_corners_lasts5_1_away'] = team1_away['team2_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team1_away['team2_corners'].isna().any() else np.nan\n",
    "\n",
    "        # Mediana e Desvio Padrão para Corners Concedidos\n",
    "        combined_df_13c.at[i, 'median_conc_corners_lasts5_1_away'] = team1_away['team1_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team1_away['team1_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_conc_corners_lasts5_1_away'] = team1_away['team1_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team1_away['team1_corners'].isna().any() else np.nan\n",
    "\n",
    "\n",
    "        # Média e Mediana do saldo da diferença de escanteios para o time 1 nas últimas 5 partidas fora de casa\n",
    "        combined_df_13c.at[i, 'avg_corners_diff_lasts5_1_away'] = team1_away['corners_diff_team2'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['corners_diff_team2'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'median_corners_diff_lasts5_1_away'] = team1_away['corners_diff_team2'].rolling(5, min_periods=2).median().iloc[-1] if not team1_away['corners_diff_team2'].isna().any() else np.nan\n",
    "\n",
    "\n",
    "    if not team2_home.empty:\n",
    "        combined_df_13c.at[i, 'avg_scr_lasts3_2_home'] = team2_home['team1_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team2_home['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_scr_lasts5_2_home'] = team2_home['team1_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_lasts3_2_home'] = team2_home['team2_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team2_home['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_lasts5_2_home'] = team2_home['team2_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team2_goals'].isna().any() else np.nan\n",
    "        ####################################### ABAIXO É SOBRE FINALIZAÇÕES ###########################################\n",
    "        combined_df_13c.at[i, 'avg_total_shots_lasts5_2_home'] = team2_home['team1_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_total_shots'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_otarget_shots_lasts5_2_home'] = team2_home['team1_shots_on_target'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_shots_on_target'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_out_shots_lasts5_2_home'] = team2_home['team1_shots_out'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_shots_out'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_total_shots_lasts5_2_home'] = team2_home['team2_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team2_total_shots'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE CORNERS ##########################################\n",
    "        combined_df_13c.at[i, 'avg_corners_lasts5_2_home'] = team2_home['team1_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_corners_conc_lasts5_2_home'] = team2_home['team2_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team2_corners'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE FOULS ##########################################\n",
    "        combined_df_13c.at[i, 'avg_fouls_lasts5_2_home'] = team2_home['team1_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_fouls'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_fouls_conc_lasts5_2_home'] = team2_home['team2_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team2_fouls'].isna().any() else np.nan\n",
    "\n",
    "        # Mediana e Desvio Padrão para Finalizações (Shots)\n",
    "        combined_df_13c.at[i, 'median_total_shots_lasts5_2_home'] = team2_home['team1_total_shots'].rolling(5, min_periods=2).median().iloc[-1] if not team2_home['team1_total_shots'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_total_shots_lasts5_2_home'] = team2_home['team1_total_shots'].rolling(5, min_periods=2).std().iloc[-1] if not team2_home['team1_total_shots'].isna().any() else np.nan\n",
    "        \n",
    "        # Mediana e Desvio Padrão para Corners\n",
    "        combined_df_13c.at[i, 'median_corners_lasts5_2_home'] = team2_home['team1_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team2_home['team1_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_corners_lasts5_2_home'] = team2_home['team1_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team2_home['team1_corners'].isna().any() else np.nan\n",
    "        # Mediana e Desvio Padrão para Corners Concedidos\n",
    "        combined_df_13c.at[i, 'median_conc_corners_lasts5_2_home'] = team2_home['team2_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team2_home['team2_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_conc_corners_lasts5_2_home'] = team2_home['team2_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team2_home['team2_corners'].isna().any() else np.nan\n",
    "\n",
    "\n",
    "        # Média e Mediana do saldo da diferença de escanteios para o time 2 nas últimas 5 partidas em casa\n",
    "        combined_df_13c.at[i, 'avg_corners_diff_lasts5_2_home'] = team2_home['corners_diff_team2'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['corners_diff_team2'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'median_corners_diff_lasts5_2_home'] = team2_home['corners_diff_team2'].rolling(5, min_periods=2).median().iloc[-1] if not team2_home['corners_diff_team2'].isna().any() else np.nan\n",
    "\n",
    "\n",
    "    if not team2_away.empty:\n",
    "        combined_df_13c.at[i, 'avg_scr_lasts3_2_away'] = team2_away['team2_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team2_away['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_scr_lasts5_2_away'] = team2_away['team2_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_lasts3_2_away'] = team2_away['team1_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team2_away['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_lasts5_2_away'] = team2_away['team1_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team1_goals'].isna().any() else np.nan\n",
    "        ####################################### ABAIXO É SOBRE FINALIZAÇÕES ###########################################\n",
    "        combined_df_13c.at[i, 'avg_total_shots_lasts5_2_away'] = team2_away['team2_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_total_shots'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_otarget_shots_lasts5_2_away'] = team2_away['team2_shots_on_target'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_shots_on_target'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_out_shots_lasts5_2_away'] = team2_away['team2_shots_out'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_shots_out'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_conc_total_shots_lasts5_2_away'] = team2_away['team1_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team1_total_shots'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE CORNERS ##########################################\n",
    "        combined_df_13c.at[i, 'avg_corners_lasts5_2_away'] = team2_away['team2_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_corners_conc_lasts5_2_away'] = team2_away['team1_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team1_corners'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE FOULS ##########################################\n",
    "        combined_df_13c.at[i, 'avg_fouls_lasts5_2_away'] = team2_away['team2_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_fouls'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'avg_fouls_conc_lasts5_2_away'] = team2_away['team1_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team1_fouls'].isna().any() else np.nan\n",
    "        \n",
    "        # Mediana e Desvio Padrão para Finalizações (Shots)\n",
    "        combined_df_13c.at[i, 'median_total_shots_lasts5_2_away'] = team2_away['team2_total_shots'].rolling(5, min_periods=2).median().iloc[-1] if not team2_away['team2_total_shots'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_total_shots_lasts5_2_away'] = team2_away['team2_total_shots'].rolling(5, min_periods=2).std().iloc[-1] if not team2_away['team2_total_shots'].isna().any() else np.nan\n",
    "        \n",
    "        # Mediana e Desvio Padrão para Corners\n",
    "        combined_df_13c.at[i, 'median_corners_lasts5_2_away'] = team2_away['team2_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team2_away['team2_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_corners_lasts5_2_away'] = team2_away['team2_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team2_away['team2_corners'].isna().any() else np.nan\n",
    "\n",
    "        # Mediana e Desvio Padrão para Corners Concedidos\n",
    "        combined_df_13c.at[i, 'median_conc_corners_lasts5_2_away'] = team2_away['team1_corners'].rolling(5, min_periods=2).median().iloc[-1] if not team2_away['team1_corners'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'std_conc_corners_lasts5_2_away'] = team2_away['team1_corners'].rolling(5, min_periods=2).std().iloc[-1] if not team2_away['team1_corners'].isna().any() else np.nan\n",
    "\n",
    "\n",
    "        # Média e Mediana do saldo da diferença de escanteios para o time 2 nas últimas 5 partidas fora de casa\n",
    "        combined_df_13c.at[i, 'avg_corners_diff_lasts5_2_away'] = team2_away['corners_diff_team2'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['corners_diff_team2'].isna().any() else np.nan\n",
    "        combined_df_13c.at[i, 'median_corners_diff_lasts5_2_away'] = team2_away['corners_diff_team2'].rolling(5, min_periods=2).median().iloc[-1] if not team2_away['corners_diff_team2'].isna().any() else np.nan\n",
    "\n",
    "\n",
    "combined_df_13c.shape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\28988986.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c[['result_team1', 'result_team2']] = combined_df_13c.apply(get_result, axis=1)\n",
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\28988986.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c[['result_team1', 'result_team2']] = combined_df_13c.apply(get_result, axis=1)\n",
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\28988986.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c.loc[team1_mask, 'team1_suspended_players'] = team_df.loc[team1_mask, 'next_match_suspended_players']\n",
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14044\\28988986.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df_13c.loc[team2_mask, 'team2_suspended_players'] = team_df.loc[team2_mask, 'next_match_suspended_players']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77062, 173)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_result(row):\n",
    "    if row['team1_goals'] > row['team2_goals']:\n",
    "        return pd.Series([3, 0])\n",
    "    elif row['team1_goals'] < row['team2_goals']:\n",
    "        return pd.Series([0, 3])\n",
    "    else:\n",
    "        return pd.Series([1, 1])\n",
    "\n",
    "combined_df_13c[['result_team1', 'result_team2']] = combined_df_13c.apply(get_result, axis=1)\n",
    "\n",
    "def get_streak(df, result_col, results):\n",
    "    result_series = df[result_col].apply(lambda x: 1 if x in results else 0)\n",
    "    result_series = result_series * (result_series.groupby((result_series != result_series.shift()).cumsum()).cumcount() + 1)\n",
    "    return result_series\n",
    "\n",
    "# Create a dictionary to hold individual team dataframes\n",
    "team_df_dict = {}\n",
    "\n",
    "def get_individual_team_df(df, team_name):\n",
    "    if team_name in team_df_dict:\n",
    "        return team_df_dict[team_name]\n",
    "        \n",
    "    team_games = df[(df['team1'] == team_name) | (df['team2'] == team_name)].copy()\n",
    "    team_games['team_is_team1'] = team_games['team1'] == team_name\n",
    "    team_games['team_result'] = np.where(team_games['team_is_team1'], team_games['result_team1'], team_games['result_team2'])\n",
    "    team_games['team_goals'] = np.where(team_games['team_is_team1'], team_games['team1_goals'], team_games['team2_goals'])\n",
    "    team_games['team_redcards'] = np.where(team_games['team_is_team1'], team_games['team1_red_cards'], team_games['team2_red_cards'])\n",
    "\n",
    "    team_games.sort_values('date', inplace=True)\n",
    "    team_games['days_since_last_game'] = team_games['date'].diff().dt.days\n",
    "\n",
    "    team_df_dict[team_name] = team_games\n",
    "    return team_games\n",
    "\n",
    "def get_team_stats(row, df):\n",
    "    team1_games = get_individual_team_df(df, row['team1'])\n",
    "    team2_games = get_individual_team_df(df, row['team2'])\n",
    "\n",
    "    # Filter to include only games that occurred before the current game\n",
    "    team1_games = team1_games[team1_games['date'] < row['date']]\n",
    "    team2_games = team2_games[team2_games['date'] < row['date']]\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    if not team1_games.empty:\n",
    "        stats['team1_winning_streak'] = get_streak(team1_games, 'team_result', [3]).iloc[-1]\n",
    "        stats['team1_undefeated_streak'] = get_streak(team1_games, 'team_result', [1, 3]).iloc[-1]\n",
    "        stats['team1_losing_streak'] = get_streak(team1_games, 'team_result', [0]).iloc[-1]\n",
    "        stats['team1_without_winning_streak'] = get_streak(team1_games, 'team_result', [0, 1]).iloc[-1]\n",
    "        stats['avg_points_lasts5_1'] = team1_games.tail(5)['team_result'].mean()\n",
    "        stats['team1_strength'] = team1_games['team_goals'].sum() / (team1_games['team1_goals'].sum() + team1_games['team2_goals'].sum() + 0.01)\n",
    "        stats['championship_points_1'] = team1_games['team_result'].sum() / len(team1_games)\n",
    "        rested_4_or_more_days_1 = team1_games.tail(1)['days_since_last_game'].values[0] >= 4\n",
    "        stats['rested_4_days_or_more_1'] = 1 if rested_4_or_more_days_1 else -1\n",
    "\n",
    "    if not team2_games.empty:\n",
    "        stats['team2_winning_streak'] = get_streak(team2_games, 'team_result', [3]).iloc[-1]\n",
    "        stats['team2_undefeated_streak'] = get_streak(team2_games, 'team_result', [1, 3]).iloc[-1]\n",
    "        stats['team2_losing_streak'] = get_streak(team2_games, 'team_result', [0]).iloc[-1]\n",
    "        stats['team2_without_winning_streak'] = get_streak(team2_games, 'team_result', [0, 1]).iloc[-1]\n",
    "        stats['avg_points_lasts5_2'] = team2_games.tail(5)['team_result'].mean()\n",
    "        stats['team2_strength'] = team2_games['team_goals'].sum() / (team2_games['team1_goals'].sum() + team2_games['team2_goals'].sum() + 0.01)\n",
    "        stats['championship_points_2'] = team2_games['team_result'].sum() / len(team2_games)\n",
    "        rested_4_or_more_days_2 = team2_games.tail(1)['days_since_last_game'].values[0] >= 4\n",
    "        stats['rested_4_days_or_more_2'] = 1 if rested_4_or_more_days_2 else -1\n",
    "\n",
    "    return pd.Series(stats)\n",
    "\n",
    "combined_df_13c = pd.concat([combined_df_13c, combined_df_13c.apply(lambda row: get_team_stats(row, combined_df_13c), axis=1)], axis=1)\n",
    "\n",
    "# Now, calculate the number of suspended players for the next match for each team.\n",
    "for team_name in team_df_dict.keys():\n",
    "    team_df = team_df_dict[team_name].copy()\n",
    "    team_df['next_match_suspended_players'] = team_df['team_redcards'].shift()\n",
    "\n",
    "    # Assign the suspended players back to the combined_df_13c.\n",
    "    team1_mask = combined_df_13c['team1'] == team_name\n",
    "    team2_mask = combined_df_13c['team2'] == team_name\n",
    "    combined_df_13c.loc[team1_mask, 'team1_suspended_players'] = team_df.loc[team1_mask, 'next_match_suspended_players']\n",
    "    combined_df_13c.loc[team2_mask, 'team2_suspended_players'] = team_df.loc[team2_mask, 'next_match_suspended_players']\n",
    "\n",
    "combined_df_13c.shape    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamento básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66840, 173)\n"
     ]
    }
   ],
   "source": [
    "# Replace empty strings with NaN\n",
    "combined_df_13c.replace('', np.nan, inplace=True)\n",
    "\n",
    "# Remove rows that contain any missing values\n",
    "combined_df_13c.dropna(inplace=True)\n",
    "\n",
    "# Convert the date column to datetime objects (Brazilian format) and handle invalid dates\n",
    "combined_df_13c['date'] = pd.to_datetime(combined_df_13c['date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "\n",
    "dataset = combined_df_13c.copy()\n",
    "\n",
    "# Sort the dataset by date\n",
    "dataset.sort_values(by='date', inplace=True)\n",
    "\n",
    "# Convert team names to lowercase\n",
    "dataset['team1'] = dataset['team1'].str.lower()\n",
    "dataset['team2'] = dataset['team2'].str.lower()\n",
    "\n",
    "# Check the first few rows of the dataset\n",
    "print(dataset.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E0' 'SC0' 'E3' 'E2' 'E1' 'I1' 'SP1' 'D1' 'F1' 'P1' 'D2' 'SP2' 'T1' 'I2'\n",
      " 'N1' 'F2' 'B1' 'G1']\n"
     ]
    }
   ],
   "source": [
    "champ_uniques = dataset['championship'].unique()\n",
    "print(champ_uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(66840, 173)\n"
     ]
    }
   ],
   "source": [
    "nan_counts = dataset.isna().sum()\n",
    "nan_tot = nan_counts.sum()\n",
    "print(nan_tot)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvar para pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_13c.to_pickle(\"dataset_173cols_europeu.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(66840, 141)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_pickle(r'C:\\Users\\mathe\\OneDrive\\Área de Trabalho\\SoccerIA\\MathIA_v7\\dataset_141cols_europeu.pkl')\n",
    "\n",
    "nan_counts = dataset.isna().sum()\n",
    "nan_tot = nan_counts.sum()\n",
    "print(nan_tot)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando os dataframes de 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe2023_BRA_A.xlsx\n",
      "(371, 19)\n",
      "team1                      0\n",
      "team2                      0\n",
      "team1_goals              182\n",
      "team2_goals              182\n",
      "season                     0\n",
      "championship               0\n",
      "team1_shots_on_target    182\n",
      "team1_shots_out          182\n",
      "team2_shots_on_target    182\n",
      "team2_shots_out          182\n",
      "team1_red_cards          182\n",
      "team2_red_cards          182\n",
      "team1_fouls              182\n",
      "team2_fouls              182\n",
      "team1_corners            182\n",
      "team2_corners            182\n",
      "team1_total_shots        182\n",
      "team2_total_shots        182\n",
      "date                       0\n",
      "dtype: int64\n",
      "dataframe2023_SUE_A.xlsx\n",
      "(233, 19)\n",
      "team1                     0\n",
      "team2                     0\n",
      "team1_goals              88\n",
      "team2_goals              88\n",
      "season                    0\n",
      "championship              0\n",
      "team1_shots_on_target    90\n",
      "team1_shots_out          90\n",
      "team2_shots_on_target    90\n",
      "team2_shots_out          90\n",
      "team1_red_cards          88\n",
      "team2_red_cards          88\n",
      "team1_fouls              88\n",
      "team2_fouls              88\n",
      "team1_corners            88\n",
      "team2_corners            88\n",
      "team1_total_shots        90\n",
      "team2_total_shots        90\n",
      "date                      0\n",
      "dtype: int64\n",
      "Index(['team1', 'team2', 'team1_goals', 'team2_goals', 'season',\n",
      "       'championship', 'team1_shots_on_target', 'team1_shots_out',\n",
      "       'team2_shots_on_target', 'team2_shots_out', 'team1_red_cards',\n",
      "       'team2_red_cards', 'team1_fouls', 'team2_fouls', 'team1_corners',\n",
      "       'team2_corners', 'team1_total_shots', 'team2_total_shots', 'date',\n",
      "       'is_future_match'],\n",
      "      dtype='object')\n",
      "(604, 20)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Caminho para a pasta que contém os arquivos .xlsx de 2023\n",
    "path = r'C:\\Users\\mathe\\OneDrive\\Área de Trabalho\\SoccerIA\\webScrappingOddspedia'\n",
    "\n",
    "# Dicionário para armazenar os dataframes\n",
    "dataframes = {}\n",
    "dfss = []\n",
    "# Lista todos os arquivos na pasta\n",
    "files = os.listdir(path)\n",
    "\n",
    "# Filtra a lista de arquivos para incluir apenas os arquivos .xlsx\n",
    "xlsx_files = [f for f in files if f.endswith('.xlsx')]\n",
    "\n",
    "# Carrega cada arquivo .xlsx em um dataframe e armazena no dicionário\n",
    "for file in xlsx_files:\n",
    "    full_path = os.path.join(path, file)  # junta o caminho do diretório com o nome do arquivo\n",
    "    dataframes[file] = pd.read_excel(full_path)  # lê o arquivo .xlsx do caminho completo\n",
    "\n",
    "    if \"Unnamed: 0\" in dataframes[file].columns:\n",
    "        dataframes[file].drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "    if \"match_report_url\" in dataframes[file].columns:\n",
    "        dataframes[file].drop(\"match_report_url\", axis=1, inplace=True)    \n",
    "\n",
    "    if \"team1_yellow_cards\" in dataframes[file].columns:\n",
    "        dataframes[file].drop(\"team1_yellow_cards\", axis=1, inplace=True)\n",
    "\n",
    "    if \"team2_yellow_cards\" in dataframes[file].columns:\n",
    "        dataframes[file].drop(\"team2_yellow_cards\", axis=1, inplace=True)  \n",
    "    dfss.append(dataframes[file])\n",
    "\n",
    "    print(file)\n",
    "    print(dataframes[file].shape)\n",
    "    nan_counts = dataframes[file].isna().sum()\n",
    "    print(nan_counts)\n",
    "\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "combined_df_2023 = pd.concat(dfss, ignore_index=True)\n",
    "\n",
    "\n",
    "combined_df_2023['team1'] = combined_df_2023['team1'].str.lower()\n",
    "combined_df_2023['team2'] = combined_df_2023['team2'].str.lower()\n",
    "combined_df_2023.replace('', np.nan, inplace=True)\n",
    "# Add a column to mark future matches\n",
    "combined_df_2023['is_future_match'] = combined_df_2023['team1_goals'].isna() | combined_df_2023['team2_goals'].isna()\n",
    "combined_df_2023['season'] = '2023'\n",
    "# Replace empty string with NaN\n",
    "combined_df_2023[\"team1_red_cards\"].replace('', np.nan, inplace=True)\n",
    "combined_df_2023[\"team2_red_cards\"].replace('', np.nan, inplace=True)\n",
    "\n",
    "# Replace NaN with 0\n",
    "combined_df_2023[\"team1_red_cards\"].fillna(0, inplace=True)\n",
    "combined_df_2023[\"team2_red_cards\"].fillna(0, inplace=True)\n",
    "\n",
    "combined_df_2023['date'] = pd.to_datetime(combined_df_2023['date'], format='%Y-%m-%d', errors='coerce')\n",
    "combined_df_2023.sort_values('date', inplace=True)\n",
    "\n",
    "print(combined_df_2023.columns)\n",
    "print(combined_df_2023.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(604, 120)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "combined_df_2023.sort_values('date', inplace=True)\n",
    "\n",
    "\n",
    "combined_df_2023['team1_goals'] = pd.to_numeric(combined_df_2023['team1_goals'], errors='coerce')\n",
    "combined_df_2023['team2_goals'] = pd.to_numeric(combined_df_2023['team2_goals'], errors='coerce')\n",
    "\n",
    "# calculate goal differences\n",
    "combined_df_2023['goal_diff_team1'] = combined_df_2023['team1_goals'] - combined_df_2023['team2_goals']\n",
    "combined_df_2023['goal_diff_team2'] = combined_df_2023['team2_goals'] - combined_df_2023['team1_goals']\n",
    "\n",
    "# calculate corners differences\n",
    "combined_df_2023['corners_diff_team1'] = combined_df_2023['team1_corners'] - combined_df_2023['team2_corners']############# NEW\n",
    "combined_df_2023['corners_diff_team2'] = combined_df_2023['team2_corners'] - combined_df_2023['team1_corners']############# NEW\n",
    "\n",
    "# calculate big wins and losses\n",
    "combined_df_2023['team1_big_win'] = np.where(combined_df_2023['goal_diff_team1'] >= 2, 1, 0)\n",
    "combined_df_2023['team1_big_loss'] = np.where(combined_df_2023['goal_diff_team1'] <= -2, 1, 0)\n",
    "combined_df_2023['team2_big_win'] = np.where(combined_df_2023['goal_diff_team2'] >= 2, 1, 0)\n",
    "combined_df_2023['team2_big_loss'] = np.where(combined_df_2023['goal_diff_team2'] <= -2, 1, 0)\n",
    "\n",
    "# calculate AH-2.5 win and losses\n",
    "combined_df_2023['team1_ah-2.5_win'] = np.where(combined_df_2023['corners_diff_team1'] >= 3, 1, 0)############# NEW\n",
    "combined_df_2023['team1_ah-2.5_loss'] = np.where(combined_df_2023['corners_diff_team1'] <= 2, 1, 0)############# NEW\n",
    "combined_df_2023['team2_ah-2.5_win'] = np.where(combined_df_2023['corners_diff_team2'] >= 3, 1, 0)############# NEW\n",
    "combined_df_2023['team2_ah-2.5_loss'] = np.where(combined_df_2023['corners_diff_team2'] <= 2, 1, 0)############# NEW\n",
    "\n",
    "\n",
    "# calculate AH+2.5 win and losses\n",
    "combined_df_2023['team1_ah+2.5_win'] = np.where(combined_df_2023['corners_diff_team1'] >= -2, 1, 0)############# NEW\n",
    "combined_df_2023['team1_ah+2.5_loss'] = np.where(combined_df_2023['corners_diff_team1'] <= -3, 1, 0)############# NEW\n",
    "combined_df_2023['team2_ah+2.5_win'] = np.where(combined_df_2023['corners_diff_team2'] >= -2, 1, 0)############# NEW\n",
    "combined_df_2023['team2_ah+2.5_loss'] = np.where(combined_df_2023['corners_diff_team2'] <= -3, 1, 0)############# NEW\n",
    "\n",
    "\n",
    "# calculate over4.5 win and losses\n",
    "combined_df_2023['team1_over4.5'] = np.where(combined_df_2023['team1_corners'] >= 5, 1, 0)############# NEW\n",
    "combined_df_2023['team1_under4.5'] = np.where(combined_df_2023['team1_corners'] <= 4, 1, 0)############# NEW\n",
    "combined_df_2023['team2_over4.5'] = np.where(combined_df_2023['team2_corners'] >= 5, 1, 0)############# NEW\n",
    "combined_df_2023['team2_under4.5'] = np.where(combined_df_2023['team2_corners'] <= 4, 1, 0)############# NEW\n",
    "\n",
    "# calculate over3.5 win and losses\n",
    "combined_df_2023['team1_over3.5'] = np.where(combined_df_2023['team1_corners'] >= 4, 1, 0)############# NEW\n",
    "combined_df_2023['team1_under3.5'] = np.where(combined_df_2023['team1_corners'] <= 3, 1, 0)############# NEW\n",
    "combined_df_2023['team2_over3.5'] = np.where(combined_df_2023['team2_corners'] >= 4, 1, 0)############# NEW\n",
    "combined_df_2023['team2_under3.5'] = np.where(combined_df_2023['team2_corners'] <= 3, 1, 0)############# NEW\n",
    "\n",
    "# calculate over6.5 win and losses\n",
    "combined_df_2023['team1_over6.5'] = np.where(combined_df_2023['team1_corners'] >= 7, 1, 0)############# NEW\n",
    "combined_df_2023['team1_under6.5'] = np.where(combined_df_2023['team1_corners'] <= 6, 1, 0)############# NEW\n",
    "combined_df_2023['team2_over6.5'] = np.where(combined_df_2023['team2_corners'] >= 7, 1, 0)############# NEW\n",
    "combined_df_2023['team2_under6.5'] = np.where(combined_df_2023['team2_corners'] <= 6, 1, 0)############# NEW\n",
    "\n",
    "\n",
    "# Initialize these columns with 0\n",
    "combined_df_2023['team1_big_wins_last5'] = 0\n",
    "combined_df_2023['team1_big_losses_last5'] = 0\n",
    "combined_df_2023['team2_big_wins_last5'] = 0\n",
    "combined_df_2023['team2_big_losses_last5'] = 0\n",
    "\n",
    "combined_df_2023['team1_ah-2.5_wins_last5'] = 0############# NEW\n",
    "combined_df_2023['team1_ah-2.5_losses_last5'] = 0############# NEW\n",
    "combined_df_2023['team2_ah-2.5_wins_last5'] = 0############# NEW\n",
    "combined_df_2023['team2_ah-2.5_losses_last5'] = 0############# NEW\n",
    "\n",
    "combined_df_2023['team1_ah+2.5_wins_last5'] = 0############# NEW\n",
    "combined_df_2023['team1_ah+2.5_losses_last5'] = 0############# NEW\n",
    "combined_df_2023['team2_ah+2.5_wins_last5'] = 0############# NEW\n",
    "combined_df_2023['team2_ah+2.5_losses_last5'] = 0############# NEW\n",
    "\n",
    "combined_df_2023['team1_over3.5_last5'] = 0############# NEW\n",
    "combined_df_2023['team1_under3.5_last5'] = 0############# NEW\n",
    "combined_df_2023['team2_over3.5_last5'] = 0############# NEW\n",
    "combined_df_2023['team2_under3.5_last5'] = 0############# NEW\n",
    "\n",
    "combined_df_2023['team1_over4.5_last5'] = 0############# NEW\n",
    "combined_df_2023['team1_under4.5_last5'] = 0############# NEW\n",
    "combined_df_2023['team2_over4.5_last5'] = 0############# NEW\n",
    "combined_df_2023['team2_under4.5_last5'] = 0############# NEW\n",
    "\n",
    "combined_df_2023['team1_over6.5_last5'] = 0############# NEW\n",
    "combined_df_2023['team1_under6.5_last5'] = 0############# NEW\n",
    "combined_df_2023['team2_over6.5_last5'] = 0############# NEW\n",
    "combined_df_2023['team2_under6.5_last5'] = 0############# NEW\n",
    "\n",
    "\n",
    "new_cols = ['avg_scr_lasts3_1_home', 'avg_scr_lasts5_1_home', 'avg_scr_lasts3_1_away',\n",
    "            'avg_scr_lasts5_1_away', 'avg_conc_lasts3_1_home', 'avg_conc_lasts5_1_home',\n",
    "            'avg_conc_lasts3_1_away', 'avg_conc_lasts5_1_away', 'avg_scr_lasts3_2_home',\n",
    "            'avg_scr_lasts5_2_home', 'avg_scr_lasts3_2_away', 'avg_scr_lasts5_2_away',\n",
    "            'avg_conc_lasts3_2_home', 'avg_conc_lasts5_2_home', 'avg_conc_lasts3_2_away',\n",
    "            'avg_conc_lasts5_2_away','team1_big_wins_last5', 'team1_big_losses_last5', \n",
    "            'team2_big_wins_last5', 'team2_big_losses_last5',\n",
    "            #abaixo vai ser baseado em finalizações\n",
    "            'avg_total_shots_lasts5_1_home','avg_total_shots_lasts5_1_away','avg_total_shots_lasts5_2_home',\n",
    "            'avg_total_shots_lasts5_2_away', 'avg_otarget_shots_lasts5_1_home','avg_otarget_shots_lasts5_1_away',\n",
    "            'avg_otarget_shots_lasts5_2_home','avg_otarget_shots_lasts5_2_away','avg_out_shots_lasts5_1_home',\n",
    "            'avg_out_shots_lasts5_1_away','avg_out_shots_lasts5_2_home','avg_out_shots_lasts5_2_away',\n",
    "            'avg_conc_total_shots_lasts5_1_home','avg_conc_total_shots_lasts5_1_away',\n",
    "            'avg_conc_total_shots_lasts5_2_home','avg_conc_total_shots_lasts5_2_away',\n",
    "            #abaixo vai ser baseado em corners\n",
    "            'avg_corners_lasts5_1_home','avg_corners_lasts5_1_away', \n",
    "            'avg_corners_conc_lasts5_1_home','avg_corners_conc_lasts5_1_away',\n",
    "            'avg_corners_lasts5_2_home','avg_corners_lasts5_2_away', \n",
    "            'avg_corners_conc_lasts5_2_home', 'avg_corners_conc_lasts5_2_away',\n",
    "            #abaixo vai ser baseado em fouls\n",
    "            'avg_fouls_lasts5_1_home','avg_fouls_lasts5_1_away', \n",
    "            'avg_fouls_conc_lasts5_1_home', 'avg_fouls_conc_lasts5_1_away',\n",
    "            'avg_fouls_lasts5_2_home','avg_fouls_lasts5_2_away', \n",
    "            'avg_fouls_conc_lasts5_2_home', 'avg_fouls_conc_lasts5_2_away',\n",
    "            #novas colunas da v7\n",
    "            'team1_ah-2.5_wins_last5', 'team1_ah-2.5_losses_last5','team2_ah-2.5_wins_last5','team2_ah-2.5_losses_last5',\n",
    "            'team1_ah+2.5_wins_last5','team1_ah+2.5_losses_last5','team2_ah+2.5_wins_last5','team2_ah+2.5_losses_last5',\n",
    "            'team1_over3.5_last5','team1_under3.5_last5','team2_over3.5_last5','team2_under3.5_last5',\n",
    "            'team1_over4.5_last5','team1_under4.5_last5','team2_over4.5_last5','team2_under4.5_last5',\n",
    "            'team1_over6.5_last5','team1_under6.5_last5','team2_over6.5_last5','team2_under6.5_last5'\n",
    "            ]\n",
    "\n",
    "# Create a dictionary with keys as column names and values as np.nan\n",
    "new_cols_dict = {col: np.nan for col in new_cols}\n",
    "\n",
    "# Add new columns to the DataFrame\n",
    "combined_df_2023 = combined_df_2023.assign(**new_cols_dict)\n",
    "\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for i, row in combined_df_2023.iterrows():\n",
    "    # For each team, get their past home and away matches before the current date\n",
    "    team1_matches = combined_df_2023[((combined_df_2023['team1'] == row['team1']) | (combined_df_2023['team2'] == row['team1'])) & (combined_df_2023['date'] < row['date']) & (combined_df_2023['season'] == row['season'])].sort_values(by='date')\n",
    "    team2_matches = combined_df_2023[((combined_df_2023['team1'] == row['team2']) | (combined_df_2023['team2'] == row['team2'])) & (combined_df_2023['date'] < row['date']) & (combined_df_2023['season'] == row['season'])].sort_values(by='date')\n",
    "\n",
    "    # For each team, calculate stats for last 5 matches\n",
    "    if not team1_matches.empty:\n",
    "        team1_matches['big_win'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_big_win'], team1_matches['team2_big_win'])\n",
    "        team1_matches['big_loss'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_big_loss'], team1_matches['team2_big_loss'])\n",
    "        combined_df_2023.at[i, 'team1_big_wins_last5'] = team1_matches['big_win'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team1_big_losses_last5'] = team1_matches['big_loss'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para ah-2.5 para a equipe 1\n",
    "        team1_matches['ah-2.5_win'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_ah-2.5_win'], team1_matches['team2_ah-2.5_win'])\n",
    "        team1_matches['ah-2.5_loss'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_ah-2.5_loss'], team1_matches['team2_ah-2.5_loss'])\n",
    "        combined_df_2023.at[i, 'team1_ah-2.5_wins_last5'] = team1_matches['ah-2.5_win'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team1_ah-2.5_losses_last5'] = team1_matches['ah-2.5_loss'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para ah+2.5 para a equipe 1\n",
    "        team1_matches['ah+2.5_win'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_ah+2.5_win'], team1_matches['team2_ah+2.5_win'])\n",
    "        team1_matches['ah+2.5_loss'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_ah+2.5_loss'], team1_matches['team2_ah+2.5_loss'])\n",
    "        combined_df_2023.at[i, 'team1_ah+2.5_wins_last5'] = team1_matches['ah+2.5_win'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team1_ah+2.5_losses_last5'] = team1_matches['ah+2.5_loss'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over3.5 para a equipe 1\n",
    "        team1_matches['over3.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_over3.5'], team1_matches['team2_over3.5'])\n",
    "        team1_matches['under3.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_under3.5'], team1_matches['team2_under3.5'])\n",
    "        combined_df_2023.at[i, 'team1_over3.5_last5'] = team1_matches['over3.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team1_under3.5_last5'] = team1_matches['under3.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over4.5 para a equipe 1\n",
    "        team1_matches['over4.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_over4.5'], team1_matches['team2_over4.5'])\n",
    "        team1_matches['under4.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_under4.5'], team1_matches['team2_under4.5'])\n",
    "        combined_df_2023.at[i, 'team1_over4.5_last5'] = team1_matches['over4.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team1_under4.5_last5'] = team1_matches['under4.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over6.5 para a equipe 1\n",
    "        team1_matches['over6.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_over6.5'], team1_matches['team2_over6.5'])\n",
    "        team1_matches['under6.5'] = np.where(team1_matches['team1'] == row['team1'], team1_matches['team1_under6.5'], team1_matches['team2_under6.5'])\n",
    "        combined_df_2023.at[i, 'team1_over6.5_last5'] = team1_matches['over6.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team1_under6.5_last5'] = team1_matches['under6.5'].rolling(5).sum().iloc[-1] if len(team1_matches) >= 2 else np.nan\n",
    "\n",
    "\n",
    "\n",
    "    if not team2_matches.empty:\n",
    "        team2_matches['big_win'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_big_win'], team2_matches['team2_big_win'])\n",
    "        team2_matches['big_loss'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_big_loss'], team2_matches['team2_big_loss'])\n",
    "        combined_df_2023.at[i, 'team2_big_wins_last5'] = team2_matches['big_win'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team2_big_losses_last5'] = team2_matches['big_loss'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para ah-2.5 para a equipe 2\n",
    "        team2_matches['ah-2.5_win'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_ah-2.5_win'], team2_matches['team2_ah-2.5_win'])\n",
    "        team2_matches['ah-2.5_loss'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_ah-2.5_loss'], team2_matches['team2_ah-2.5_loss'])\n",
    "        combined_df_2023.at[i, 'team2_ah-2.5_wins_last5'] = team2_matches['ah-2.5_win'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team2_ah-2.5_losses_last5'] = team2_matches['ah-2.5_loss'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para ah+2.5 para a equipe 2\n",
    "        team2_matches['ah+2.5_win'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_ah+2.5_win'], team2_matches['team2_ah+2.5_win'])\n",
    "        team2_matches['ah+2.5_loss'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_ah+2.5_loss'], team2_matches['team2_ah+2.5_loss'])\n",
    "        combined_df_2023.at[i, 'team2_ah+2.5_wins_last5'] = team2_matches['ah+2.5_win'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team2_ah+2.5_losses_last5'] = team2_matches['ah+2.5_loss'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over3.5  para a equipe 2\n",
    "        team2_matches['over3.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_over3.5'], team2_matches['team2_over3.5'])\n",
    "        team2_matches['under3.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_under3.5'], team2_matches['team2_under3.5'])\n",
    "        combined_df_2023.at[i, 'team2_over3.5_last5'] = team2_matches['over3.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team2_under3.5_last5'] = team2_matches['under3.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over4.5  para a equipe 2\n",
    "        team2_matches['over4.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_over4.5'], team2_matches['team2_over4.5'])\n",
    "        team2_matches['under4.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_under4.5'], team2_matches['team2_under4.5'])\n",
    "        combined_df_2023.at[i, 'team2_over4.5_last5'] = team2_matches['over4.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team2_under4.5_last5'] = team2_matches['under4.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "        # Para over6.5  para a equipe 2\n",
    "        team2_matches['over6.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_over6.5'], team2_matches['team2_over6.5'])\n",
    "        team2_matches['under6.5'] = np.where(team2_matches['team1'] == row['team2'], team2_matches['team1_under6.5'], team2_matches['team2_under6.5'])\n",
    "        combined_df_2023.at[i, 'team2_over6.5_last5'] = team2_matches['over6.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "        combined_df_2023.at[i, 'team2_under6.5_last5'] = team2_matches['under6.5'].rolling(5).sum().iloc[-1] if len(team2_matches) >= 2 else np.nan\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for i, row in combined_df_2023.iterrows():\n",
    "    team1_home = combined_df_2023[(combined_df_2023['date'] < row['date']) & (combined_df_2023['team1'] == row['team1']) & (combined_df_2023['season'] == row['season'])]\n",
    "    team1_away = combined_df_2023[(combined_df_2023['date'] < row['date']) & (combined_df_2023['team2'] == row['team1']) & (combined_df_2023['season'] == row['season'])]\n",
    "    \n",
    "    team2_home = combined_df_2023[(combined_df_2023['date'] < row['date']) & (combined_df_2023['team1'] == row['team2']) & (combined_df_2023['season'] == row['season'])]\n",
    "    team2_away = combined_df_2023[(combined_df_2023['date'] < row['date']) & (combined_df_2023['team2'] == row['team2']) & (combined_df_2023['season'] == row['season'])]\n",
    "\n",
    "    if not team1_home.empty:\n",
    "        combined_df_2023.at[i, 'avg_scr_lasts3_1_home'] = team1_home['team1_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team1_home['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_scr_lasts5_1_home'] = team1_home['team1_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_lasts3_1_home'] = team1_home['team2_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team1_home['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_lasts5_1_home'] = team1_home['team2_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team2_goals'].isna().any() else np.nan\n",
    "        ####################################### ABAIXO É SOBRE FINALIZAÇÕES ###########################################\n",
    "        combined_df_2023.at[i, 'avg_total_shots_lasts5_1_home'] = team1_home['team1_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_total_shots'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_total_shots_lasts5_1_home'] = team1_home['team2_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team2_total_shots'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_otarget_shots_lasts5_1_home'] = team1_home['team1_shots_on_target'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_shots_on_target'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_out_shots_lasts5_1_home'] = team1_home['team1_shots_out'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_shots_out'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE CORNERS ##########################################\n",
    "        combined_df_2023.at[i, 'avg_corners_lasts5_1_home'] = team1_home['team1_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_corners'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_corners_conc_lasts5_1_home'] = team1_home['team2_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team2_corners'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE FOULS ##########################################\n",
    "        combined_df_2023.at[i, 'avg_fouls_lasts5_1_home'] = team1_home['team1_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team1_fouls'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_fouls_conc_lasts5_1_home'] = team1_home['team2_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_home['team2_fouls'].isna().any() else np.nan\n",
    "\n",
    "    if not team1_away.empty:\n",
    "        combined_df_2023.at[i, 'avg_scr_lasts3_1_away'] = team1_away['team2_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team1_away['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_scr_lasts5_1_away'] = team1_away['team2_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_lasts3_1_away'] = team1_away['team1_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team1_away['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_lasts5_1_away'] = team1_away['team1_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team1_goals'].isna().any() else np.nan\n",
    "        ####################################### ABAIXO É SOBRE FINALIZAÇÕES ###########################################\n",
    "        combined_df_2023.at[i, 'avg_total_shots_lasts5_1_away'] = team1_away['team2_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_total_shots'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_otarget_shots_lasts5_1_away'] = team1_away['team2_shots_on_target'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_shots_on_target'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_out_shots_lasts5_1_away'] = team1_away['team2_shots_out'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_shots_out'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_total_shots_lasts5_1_away'] = team1_away['team1_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team1_total_shots'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE CORNERS ##########################################\n",
    "        combined_df_2023.at[i, 'avg_corners_lasts5_1_away'] = team1_away['team2_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_corners'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_corners_conc_lasts5_1_away'] = team1_away['team1_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team1_corners'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE FOULS ##########################################\n",
    "        combined_df_2023.at[i, 'avg_fouls_lasts5_1_away'] = team1_away['team2_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team2_fouls'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_fouls_conc_lasts5_1_away'] = team1_away['team1_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team1_away['team1_fouls'].isna().any() else np.nan\n",
    "\n",
    "    if not team2_home.empty:\n",
    "        combined_df_2023.at[i, 'avg_scr_lasts3_2_home'] = team2_home['team1_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team2_home['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_scr_lasts5_2_home'] = team2_home['team1_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_lasts3_2_home'] = team2_home['team2_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team2_home['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_lasts5_2_home'] = team2_home['team2_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team2_goals'].isna().any() else np.nan\n",
    "        ####################################### ABAIXO É SOBRE FINALIZAÇÕES ###########################################\n",
    "        combined_df_2023.at[i, 'avg_total_shots_lasts5_2_home'] = team2_home['team1_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_total_shots'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_otarget_shots_lasts5_2_home'] = team2_home['team1_shots_on_target'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_shots_on_target'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_out_shots_lasts5_2_home'] = team2_home['team1_shots_out'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_shots_out'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_total_shots_lasts5_2_home'] = team2_home['team2_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team2_total_shots'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE CORNERS ##########################################\n",
    "        combined_df_2023.at[i, 'avg_corners_lasts5_2_home'] = team2_home['team1_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_corners'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_corners_conc_lasts5_2_home'] = team2_home['team2_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team2_corners'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE FOULS ##########################################\n",
    "        combined_df_2023.at[i, 'avg_fouls_lasts5_2_home'] = team2_home['team1_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team1_fouls'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_fouls_conc_lasts5_2_home'] = team2_home['team2_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_home['team2_fouls'].isna().any() else np.nan\n",
    "\n",
    "\n",
    "    if not team2_away.empty:\n",
    "        combined_df_2023.at[i, 'avg_scr_lasts3_2_away'] = team2_away['team2_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team2_away['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_scr_lasts5_2_away'] = team2_away['team2_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_lasts3_2_away'] = team2_away['team1_goals'].rolling(3, min_periods=2).mean().iloc[-1] if not team2_away['team1_goals'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_lasts5_2_away'] = team2_away['team1_goals'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team1_goals'].isna().any() else np.nan\n",
    "        ####################################### ABAIXO É SOBRE FINALIZAÇÕES ###########################################\n",
    "        combined_df_2023.at[i, 'avg_total_shots_lasts5_2_away'] = team2_away['team2_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_total_shots'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_otarget_shots_lasts5_2_away'] = team2_away['team2_shots_on_target'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_shots_on_target'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_out_shots_lasts5_2_away'] = team2_away['team2_shots_out'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_shots_out'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_conc_total_shots_lasts5_2_away'] = team2_away['team1_total_shots'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team1_total_shots'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE CORNERS ##########################################\n",
    "        combined_df_2023.at[i, 'avg_corners_lasts5_2_away'] = team2_away['team2_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_corners'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_corners_conc_lasts5_2_away'] = team2_away['team1_corners'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team1_corners'].isna().any() else np.nan\n",
    "        ########################################## ABAIXO É SOBRE FOULS ##########################################\n",
    "        combined_df_2023.at[i, 'avg_fouls_lasts5_2_away'] = team2_away['team2_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team2_fouls'].isna().any() else np.nan\n",
    "        combined_df_2023.at[i, 'avg_fouls_conc_lasts5_2_away'] = team2_away['team1_fouls'].rolling(5, min_periods=2).mean().iloc[-1] if not team2_away['team1_fouls'].isna().any() else np.nan\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined_df_2023.shape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(604, 140)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_result(row):\n",
    "    if row['team1_goals'] > row['team2_goals']:\n",
    "        return pd.Series([3, 0])\n",
    "    elif row['team1_goals'] < row['team2_goals']:\n",
    "        return pd.Series([0, 3])\n",
    "    else:\n",
    "        return pd.Series([1, 1])\n",
    "\n",
    "combined_df_2023[['result_team1', 'result_team2']] = combined_df_2023.apply(get_result, axis=1)\n",
    "\n",
    "def get_streak(df, result_col, results):\n",
    "    result_series = df[result_col].apply(lambda x: 1 if x in results else 0)\n",
    "    result_series = result_series * (result_series.groupby((result_series != result_series.shift()).cumsum()).cumcount() + 1)\n",
    "    return result_series\n",
    "\n",
    "# Create a dictionary to hold individual team dataframes\n",
    "team_df_dict = {}\n",
    "\n",
    "def get_individual_team_df(df, team_name): #teoricamente aqui deveria ser corners, mas o erro se apresentou menor assim:\n",
    "    if team_name in team_df_dict:\n",
    "        return team_df_dict[team_name]\n",
    "        \n",
    "    team_games = df[(df['team1'] == team_name) | (df['team2'] == team_name)].copy()\n",
    "    team_games['team_is_team1'] = team_games['team1'] == team_name\n",
    "    team_games['team_result'] = np.where(team_games['team_is_team1'], team_games['result_team1'], team_games['result_team2'])\n",
    "    team_games['team_goals'] = np.where(team_games['team_is_team1'], team_games['team1_goals'], team_games['team2_goals'])\n",
    "    team_games['team_redcards'] = np.where(team_games['team_is_team1'], team_games['team1_red_cards'], team_games['team2_red_cards'])\n",
    "\n",
    "    team_games.sort_values('date', inplace=True)\n",
    "    team_games['days_since_last_game'] = team_games['date'].diff().dt.days\n",
    "\n",
    "    team_df_dict[team_name] = team_games\n",
    "    return team_games\n",
    "\n",
    "def get_team_stats(row, df):\n",
    "    team1_games = get_individual_team_df(df, row['team1'])\n",
    "    team2_games = get_individual_team_df(df, row['team2'])\n",
    "\n",
    "    # Filter to include only games that occurred before the current game\n",
    "    team1_games = team1_games[team1_games['date'] < row['date']]\n",
    "    team2_games = team2_games[team2_games['date'] < row['date']]\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    if not team1_games.empty:\n",
    "        stats['team1_winning_streak'] = get_streak(team1_games, 'team_result', [3]).iloc[-1]\n",
    "        stats['team1_undefeated_streak'] = get_streak(team1_games, 'team_result', [1, 3]).iloc[-1]\n",
    "        stats['team1_losing_streak'] = get_streak(team1_games, 'team_result', [0]).iloc[-1]\n",
    "        stats['team1_without_winning_streak'] = get_streak(team1_games, 'team_result', [0, 1]).iloc[-1]\n",
    "        stats['avg_points_lasts5_1'] = team1_games.tail(5)['team_result'].mean()\n",
    "        stats['team1_strength'] = team1_games['team_goals'].sum() / (team1_games['team1_goals'].sum() + team1_games['team2_goals'].sum() + 0.01)\n",
    "        stats['championship_points_1'] = team1_games['team_result'].sum() / len(team1_games)\n",
    "        rested_4_or_more_days_1 = team1_games.tail(1)['days_since_last_game'].values[0] >= 4\n",
    "        stats['rested_4_days_or_more_1'] = 1 if rested_4_or_more_days_1 else -1\n",
    "\n",
    "    if not team2_games.empty:\n",
    "        stats['team2_winning_streak'] = get_streak(team2_games, 'team_result', [3]).iloc[-1]\n",
    "        stats['team2_undefeated_streak'] = get_streak(team2_games, 'team_result', [1, 3]).iloc[-1]\n",
    "        stats['team2_losing_streak'] = get_streak(team2_games, 'team_result', [0]).iloc[-1]\n",
    "        stats['team2_without_winning_streak'] = get_streak(team2_games, 'team_result', [0, 1]).iloc[-1]\n",
    "        stats['avg_points_lasts5_2'] = team2_games.tail(5)['team_result'].mean()\n",
    "        stats['team2_strength'] = team2_games['team_goals'].sum() / (team2_games['team1_goals'].sum() + team2_games['team2_goals'].sum() + 0.01)\n",
    "        stats['championship_points_2'] = team2_games['team_result'].sum() / len(team2_games)\n",
    "        rested_4_or_more_days_2 = team2_games.tail(1)['days_since_last_game'].values[0] >= 4\n",
    "        stats['rested_4_days_or_more_2'] = 1 if rested_4_or_more_days_2 else -1\n",
    "\n",
    "    return pd.Series(stats)\n",
    "\n",
    "combined_df_2023 = pd.concat([combined_df_2023, combined_df_2023.apply(lambda row: get_team_stats(row, combined_df_2023), axis=1)], axis=1)\n",
    "\n",
    "# Now, calculate the number of suspended players for the next match for each team.\n",
    "for team_name in team_df_dict.keys():\n",
    "    team_df = team_df_dict[team_name].copy()\n",
    "    team_df['next_match_suspended_players'] = team_df['team_redcards'].shift()\n",
    "\n",
    "    # Assign the suspended players back to the combined_df_2023.\n",
    "    team1_mask = combined_df_2023['team1'] == team_name\n",
    "    team2_mask = combined_df_2023['team2'] == team_name\n",
    "    combined_df_2023.loc[team1_mask, 'team1_suspended_players'] = team_df.loc[team1_mask, 'next_match_suspended_players']\n",
    "    combined_df_2023.loc[team2_mask, 'team2_suspended_players'] = team_df.loc[team2_mask, 'next_match_suspended_players']\n",
    "\n",
    "combined_df_2023.shape    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEPARANDO AS CLUSTERIZAÇÕES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358817, 139)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Filtrando as linhas com 'championship'\n",
    "ALE_A_df = dataset[dataset['championship'].isin(['D1', 'D2', 'T1', 'B1', 'N1'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ALE_A_df['championship'] = 'ALE A'\n",
    "##############################################################################################################\n",
    "# Filtrando as linhas com 'championship'\n",
    "ALE_B_df = dataset[dataset['championship'].isin(['D1', 'D2', 'T1', 'B1', 'N1'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ALE_B_df['championship'] = 'ALE B'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "BEL_A_df = dataset[dataset['championship'].isin(['B1', 'D1', 'D2', 'T1'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "BEL_A_df['championship'] = 'BEL A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "ESC_A_df = dataset[dataset['championship'].isin(['SC0', 'E2', 'E3', 'P1', 'E1'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ESC_A_df['championship'] = 'ESC A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "ESP_A_df = dataset[dataset['championship'].isin(['SP1','I1'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ESP_A_df['championship'] = 'ESP A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "ESP_B_df = dataset[dataset['championship'].isin(['SP2', 'F1', 'F2'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ESP_B_df['championship'] = 'ESP B'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "FRA_A_df = dataset[dataset['championship'].isin(['F1', 'I2', 'SP1', 'I1'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "FRA_A_df['championship'] = 'FRA A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "FRA_B_df = dataset[dataset['championship'].isin(['F2', 'SP2', 'G1'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "FRA_B_df['championship'] = 'FRA B'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "GRE_A_df = dataset[dataset['championship'].isin(['G1', 'F2'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "GRE_A_df['championship'] = 'GRE A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "HOL_A_df = dataset[dataset['championship'].isin(['D1', 'D2'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "HOL_A_df['championship'] = 'HOL A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "ING_A_df = dataset[dataset['championship'].isin(['E0', 'E1', 'E2', 'I1'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ING_A_df['championship'] = 'ING A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "ING_B_df = dataset[dataset['championship'].isin(['E1', 'E0', 'E2', 'E3', 'I1', 'P1', 'SC0'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ING_B_df['championship'] = 'ING B'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "ING_C_df = dataset[dataset['championship'].isin(['E2', 'E3', 'E1', 'SC0', 'P1', 'E0'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ING_C_df['championship'] = 'ING C'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "ING_D_df = dataset[dataset['championship'].isin(['E3', 'E2', 'SC0', 'E1', 'P1'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ING_D_df['championship'] = 'ING D'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "ITA_A_df = dataset[dataset['championship'].isin(['I1', 'P1', 'E1', 'E0', 'I2'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ITA_A_df['championship'] = 'ITA A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "ITA_B_df = dataset[dataset['championship'].isin(['I2', 'P1', 'F1', 'I2'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "ITA_B_df['championship'] = 'ITA B'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "POR_A_df = dataset[dataset['championship'].isin(['P1', 'E3', 'I2', 'I1', 'E1', 'E2', 'E3','SC0'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "POR_A_df['championship'] = 'POR A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "TUR_A_df = dataset[dataset['championship'].isin(['T1', 'B1', 'D1'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "TUR_A_df['championship'] = 'TUR A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "BRA_A_df = dataset[dataset['championship'].isin(['E3', 'E2', 'SC0'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "BRA_A_df['championship'] = 'BRA A'\n",
    "##############################################################################################################\n",
    "\n",
    "# Filtrando as linhas com 'championship'\n",
    "SUE_A_df = dataset[dataset['championship'].isin(['E3', 'E2', 'SC0'])].copy()\n",
    "\n",
    "# Modificando os valores da coluna 'championship'\n",
    "SUE_A_df['championship'] = 'SUE A'\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Concatenando o DataFrame original com o novo DataFrame\n",
    "combined_df_13c_new = pd.concat([\n",
    "    ALE_A_df, ALE_B_df, BEL_A_df, ESC_A_df, ESP_A_df, ESP_B_df, \n",
    "    FRA_A_df, FRA_B_df, GRE_A_df, HOL_A_df, ING_A_df, ING_B_df,\n",
    "    ING_C_df, ING_D_df, ITA_A_df, ITA_B_df, POR_A_df, TUR_A_df,\n",
    "    BRA_A_df, SUE_A_df\n",
    "                        ], ignore_index=True)\n",
    "\n",
    "if 'team2_yellow_cards' in combined_df_13c_new.columns:\n",
    "    combined_df_13c_new.drop(['team1_yellow_cards'], axis=1, inplace=True)\n",
    "\n",
    "if 'team2_yellow_cards' in combined_df_13c_new.columns:\n",
    "    combined_df_13c_new.drop(['team2_yellow_cards'], axis=1, inplace=True)\n",
    "\n",
    "# Verificando o resultado\n",
    "print(combined_df_13c_new.shape)\n",
    "nan_counts = combined_df_13c_new.isna().sum()\n",
    "print(list(nan_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conferindo se tem colunas diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "unique_to_df = set(combined_df_13c_new.columns) - set(combined_df_2023.columns)\n",
    "print(unique_to_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_future_match'}\n"
     ]
    }
   ],
   "source": [
    "unique_to_df = set(combined_df_2023.columns) - set(combined_df_13c_new.columns)\n",
    "print(unique_to_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['team1', 'team2', 'team1_goals', 'team2_goals', 'season', 'championship', 'team1_shots_on_target', 'team1_shots_out', 'team2_shots_on_target', 'team2_shots_out', 'team1_red_cards', 'team2_red_cards', 'team1_fouls', 'team2_fouls', 'team1_corners', 'team2_corners', 'team1_total_shots', 'team2_total_shots', 'date', 'is_future_match', 'goal_diff_team1', 'goal_diff_team2', 'corners_diff_team1', 'corners_diff_team2', 'team1_big_win', 'team1_big_loss', 'team2_big_win', 'team2_big_loss', 'team1_ah-2.5_win', 'team1_ah-2.5_loss', 'team2_ah-2.5_win', 'team2_ah-2.5_loss', 'team1_ah+2.5_win', 'team1_ah+2.5_loss', 'team2_ah+2.5_win', 'team2_ah+2.5_loss', 'team1_over4.5', 'team1_under4.5', 'team2_over4.5', 'team2_under4.5', 'team1_over3.5', 'team1_under3.5', 'team2_over3.5', 'team2_under3.5', 'team1_over6.5', 'team1_under6.5', 'team2_over6.5', 'team2_under6.5', 'team1_big_wins_last5', 'team1_big_losses_last5', 'team2_big_wins_last5', 'team2_big_losses_last5', 'team1_ah-2.5_wins_last5', 'team1_ah-2.5_losses_last5', 'team2_ah-2.5_wins_last5', 'team2_ah-2.5_losses_last5', 'team1_ah+2.5_wins_last5', 'team1_ah+2.5_losses_last5', 'team2_ah+2.5_wins_last5', 'team2_ah+2.5_losses_last5', 'team1_over3.5_last5', 'team1_under3.5_last5', 'team2_over3.5_last5', 'team2_under3.5_last5', 'team1_over4.5_last5', 'team1_under4.5_last5', 'team2_over4.5_last5', 'team2_under4.5_last5', 'team1_over6.5_last5', 'team1_under6.5_last5', 'team2_over6.5_last5', 'team2_under6.5_last5', 'avg_scr_lasts3_1_home', 'avg_scr_lasts5_1_home', 'avg_scr_lasts3_1_away', 'avg_scr_lasts5_1_away', 'avg_conc_lasts3_1_home', 'avg_conc_lasts5_1_home', 'avg_conc_lasts3_1_away', 'avg_conc_lasts5_1_away', 'avg_scr_lasts3_2_home', 'avg_scr_lasts5_2_home', 'avg_scr_lasts3_2_away', 'avg_scr_lasts5_2_away', 'avg_conc_lasts3_2_home', 'avg_conc_lasts5_2_home', 'avg_conc_lasts3_2_away', 'avg_conc_lasts5_2_away', 'avg_total_shots_lasts5_1_home', 'avg_total_shots_lasts5_1_away', 'avg_total_shots_lasts5_2_home', 'avg_total_shots_lasts5_2_away', 'avg_otarget_shots_lasts5_1_home', 'avg_otarget_shots_lasts5_1_away', 'avg_otarget_shots_lasts5_2_home', 'avg_otarget_shots_lasts5_2_away', 'avg_out_shots_lasts5_1_home', 'avg_out_shots_lasts5_1_away', 'avg_out_shots_lasts5_2_home', 'avg_out_shots_lasts5_2_away', 'avg_conc_total_shots_lasts5_1_home', 'avg_conc_total_shots_lasts5_1_away', 'avg_conc_total_shots_lasts5_2_home', 'avg_conc_total_shots_lasts5_2_away', 'avg_corners_lasts5_1_home', 'avg_corners_lasts5_1_away', 'avg_corners_conc_lasts5_1_home', 'avg_corners_conc_lasts5_1_away', 'avg_corners_lasts5_2_home', 'avg_corners_lasts5_2_away', 'avg_corners_conc_lasts5_2_home', 'avg_corners_conc_lasts5_2_away', 'avg_fouls_lasts5_1_home', 'avg_fouls_lasts5_1_away', 'avg_fouls_conc_lasts5_1_home', 'avg_fouls_conc_lasts5_1_away', 'avg_fouls_lasts5_2_home', 'avg_fouls_lasts5_2_away', 'avg_fouls_conc_lasts5_2_home', 'avg_fouls_conc_lasts5_2_away', 'result_team1', 'result_team2', 'avg_points_lasts5_1', 'avg_points_lasts5_2', 'championship_points_1', 'championship_points_2', 'rested_4_days_or_more_1', 'rested_4_days_or_more_2', 'team1_losing_streak', 'team1_strength', 'team1_undefeated_streak', 'team1_winning_streak', 'team1_without_winning_streak', 'team2_losing_streak', 'team2_strength', 'team2_undefeated_streak', 'team2_winning_streak', 'team2_without_winning_streak', 'team1_suspended_players', 'team2_suspended_players']\n"
     ]
    }
   ],
   "source": [
    "print(list(combined_df_2023.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o future match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 140)\n",
      "(10, 140)\n"
     ]
    }
   ],
   "source": [
    "dataset2 = combined_df_2023.copy()\n",
    "\n",
    "# filter the DataFrame\n",
    "future_matches = dataset2[dataset2['is_future_match'] == True]\n",
    "future_matches.sort_values(by='date')\n",
    "print(future_matches.shape)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get yesterday's date\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "\n",
    "# Filter the DataFrame to include only rows with dates greater than yesterday\n",
    "future_matches = future_matches[future_matches['date'] > yesterday]\n",
    "\n",
    "# Liste todas as colunas que você deseja verificar\n",
    "columns_to_check = [col for col in future_matches.columns if col not in ['team1_goals', 'team2_goals','goal_diff_team1','goal_diff_team2', 'team1_corners', 'team2_corners', 'team1_fouls', 'team2_fouls',\n",
    "                                'team1_shots_on_target', 'team1_total_shots', 'team1_shots_out', 'team2_shots_on_target', 'team2_total_shots', 'team2_shots_out',\n",
    "                                'corners_diff_team1', 'corners_diff_team2','team1_ah-2.5_win','team1_ah-2.5_loss','team2_ah-2.5_win','team2_ah-2.5_loss','team1_ah+2.5_win','team1_ah+2.5_loss','team2_ah+2.5_win','team2_ah+2.5_loss','team1_over3.5','team1_under3.5','team2_over3.5','team2_under3.5','team1_over4.5','team1_under4.5','team2_over4.5','team2_under4.5','team1_over6.5','team1_under6.5','team2_over6.5','team2_under6.5']]\n",
    "\n",
    "# Drop as linhas com 'np.nan' nas colunas especificadas\n",
    "future_matches = future_matches.dropna(subset=columns_to_check)\n",
    "\n",
    "\"\"\"# Contando os valores NaN em cada coluna\n",
    "nan_counts = future_matches.isna().sum()\n",
    "\n",
    "# Transformando em uma lista de pares (nome da coluna, contagem de np.nan)\n",
    "nan_list = list(nan_counts.items())\n",
    "\n",
    "# Percorrendo a lista e imprimindo cada valor individualmente com o nome da coluna\n",
    "for col_name, nan_count in nan_list:\n",
    "    print(f'{col_name}: {nan_count}')\"\"\"\n",
    "    \n",
    "print(future_matches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604, 140)\n",
      "A quantidade de np.nan em linhas eram 21352\n",
      "Total number of rows with 'NaN' or an empty value: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(239, 140)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CORTAR AS LINHAS COM MATCHES FUTUROS AQUI\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "print(dataset2.shape)\n",
    "dataset2.replace('', np.nan, inplace=True)\n",
    "print(f\"A quantidade de np.nan em linhas eram {dataset2.isna().sum().sum()}\")\n",
    "# Remove rows that contain any missing values\n",
    "dataset2.dropna(inplace=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "counter = 0  # Initialize counter\n",
    "for index, row in dataset2.iterrows():\n",
    "    if row.isnull().any() or row.eq('').any():\n",
    "        print(f\"Row {index} contains 'NaN' or an empty value.\")\n",
    "        counter += 1  # Increase counter if condition is met\n",
    "\n",
    "print(f\"Total number of rows with 'NaN' or an empty value: {counter}\")\n",
    "dataset2.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenando os 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team1</th>\n",
       "      <th>team2</th>\n",
       "      <th>team1_goals</th>\n",
       "      <th>team2_goals</th>\n",
       "      <th>season</th>\n",
       "      <th>championship</th>\n",
       "      <th>team1_shots_on_target</th>\n",
       "      <th>team1_shots_out</th>\n",
       "      <th>team2_shots_on_target</th>\n",
       "      <th>team2_shots_out</th>\n",
       "      <th>...</th>\n",
       "      <th>team1_undefeated_streak</th>\n",
       "      <th>team1_winning_streak</th>\n",
       "      <th>team1_without_winning_streak</th>\n",
       "      <th>team2_losing_streak</th>\n",
       "      <th>team2_strength</th>\n",
       "      <th>team2_undefeated_streak</th>\n",
       "      <th>team2_winning_streak</th>\n",
       "      <th>team2_without_winning_streak</th>\n",
       "      <th>team1_suspended_players</th>\n",
       "      <th>team2_suspended_players</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242365</th>\n",
       "      <td>Leeds</td>\n",
       "      <td>Man United</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>ITA A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132279</th>\n",
       "      <td>Leeds</td>\n",
       "      <td>Man United</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>ING B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132280</th>\n",
       "      <td>West Brom</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>ING B</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.332963</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102929</th>\n",
       "      <td>Charlton</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>ING A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666297</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102930</th>\n",
       "      <td>Everton</td>\n",
       "      <td>Middlesbrough</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>ING A</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.665927</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>vasco da gama</td>\n",
       "      <td>atlético mineiro</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>BRA A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.563958</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>coritiba</td>\n",
       "      <td>flamengo</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>BRA A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.565931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>bahia</td>\n",
       "      <td>red bull bragantino</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>BRA A</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.613497</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>santos</td>\n",
       "      <td>grêmio</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>BRA A</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>goiás</td>\n",
       "      <td>athletico-pr</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>BRA A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555432</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359056 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                team1                team2  team1_goals  team2_goals season  \\\n",
       "242365          Leeds           Man United          1.0          0.0   2002   \n",
       "132279          Leeds           Man United          1.0          0.0   2002   \n",
       "132280      West Brom          Southampton          1.0          0.0   2002   \n",
       "102929       Charlton              Arsenal          0.0          3.0   2002   \n",
       "102930        Everton        Middlesbrough          2.0          1.0   2002   \n",
       "...               ...                  ...          ...          ...    ...   \n",
       "234     vasco da gama     atlético mineiro          1.0          0.0   2023   \n",
       "235          coritiba             flamengo          2.0          3.0   2023   \n",
       "236             bahia  red bull bragantino          4.0          0.0   2023   \n",
       "237            santos               grêmio          2.0          1.0   2023   \n",
       "238             goiás         athletico-pr          1.0          1.0   2023   \n",
       "\n",
       "       championship  team1_shots_on_target  team1_shots_out  \\\n",
       "242365        ITA A                    2.0              6.0   \n",
       "132279        ING B                    2.0              6.0   \n",
       "132280        ING B                    7.0              4.0   \n",
       "102929        ING A                    3.0              6.0   \n",
       "102930        ING A                    8.0              5.0   \n",
       "...             ...                    ...              ...   \n",
       "234           BRA A                    3.0              9.0   \n",
       "235           BRA A                    4.0              7.0   \n",
       "236           BRA A                    6.0              3.0   \n",
       "237           BRA A                    8.0              7.0   \n",
       "238           BRA A                    4.0              9.0   \n",
       "\n",
       "        team2_shots_on_target  team2_shots_out  ...  team1_undefeated_streak  \\\n",
       "242365                    5.0              1.0  ...                      1.0   \n",
       "132279                    5.0              1.0  ...                      1.0   \n",
       "132280                    5.0              5.0  ...                      2.0   \n",
       "102929                    8.0              2.0  ...                      0.0   \n",
       "102930                    5.0              5.0  ...                      0.0   \n",
       "...                       ...              ...  ...                      ...   \n",
       "234                       5.0              4.0  ...                      2.0   \n",
       "235                       5.0              8.0  ...                      0.0   \n",
       "236                       8.0              2.0  ...                      0.0   \n",
       "237                       4.0              2.0  ...                      0.0   \n",
       "238                       5.0              6.0  ...                      5.0   \n",
       "\n",
       "        team1_winning_streak  team1_without_winning_streak  \\\n",
       "242365                   1.0                           0.0   \n",
       "132279                   1.0                           0.0   \n",
       "132280                   2.0                           0.0   \n",
       "102929                   0.0                           1.0   \n",
       "102930                   0.0                           3.0   \n",
       "...                      ...                           ...   \n",
       "234                      0.0                           1.0   \n",
       "235                      0.0                           3.0   \n",
       "236                      0.0                           7.0   \n",
       "237                      0.0                           5.0   \n",
       "238                      2.0                           0.0   \n",
       "\n",
       "        team2_losing_streak  team2_strength  team2_undefeated_streak  \\\n",
       "242365                  1.0        0.554939                      0.0   \n",
       "132279                  1.0        0.554939                      0.0   \n",
       "132280                  0.0        0.332963                      1.0   \n",
       "102929                  0.0        0.666297                      5.0   \n",
       "102930                  0.0        0.665927                      1.0   \n",
       "...                     ...             ...                      ...   \n",
       "234                     0.0        0.563958                      2.0   \n",
       "235                     0.0        0.565931                      1.0   \n",
       "236                     0.0        0.613497                      4.0   \n",
       "237                     0.0        0.531802                      1.0   \n",
       "238                     0.0        0.555432                      5.0   \n",
       "\n",
       "        team2_winning_streak  team2_without_winning_streak  \\\n",
       "242365                   0.0                           1.0   \n",
       "132279                   0.0                           1.0   \n",
       "132280                   1.0                           0.0   \n",
       "102929                   1.0                           0.0   \n",
       "102930                   1.0                           0.0   \n",
       "...                      ...                           ...   \n",
       "234                      2.0                           0.0   \n",
       "235                      0.0                           2.0   \n",
       "236                      0.0                           1.0   \n",
       "237                      1.0                           0.0   \n",
       "238                      1.0                           0.0   \n",
       "\n",
       "       team1_suspended_players team2_suspended_players  \n",
       "242365                     0.0                     0.0  \n",
       "132279                     0.0                     0.0  \n",
       "132280                     0.0                     0.0  \n",
       "102929                     0.0                     0.0  \n",
       "102930                     0.0                     0.0  \n",
       "...                        ...                     ...  \n",
       "234                        0.0                     0.0  \n",
       "235                        0.0                     0.0  \n",
       "236                        0.0                     0.0  \n",
       "237                        0.0                     0.0  \n",
       "238                        0.0                     0.0  \n",
       "\n",
       "[359056 rows x 140 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatotal = pd.concat([dataset2, combined_df_13c_new], ignore_index=True)\n",
    "\n",
    "datatotal.sort_values(by='date', inplace=True)\n",
    "\n",
    "if 'team1_yellow_cards' in datatotal.columns:\n",
    "    datatotal = datatotal.drop(['team1_yellow_cards'], axis=1)\n",
    "\n",
    "if 'team2_yellow_cards' in datatotal.columns:\n",
    "    datatotal = datatotal.drop(['team2_yellow_cards'], axis=1)\n",
    "\n",
    "# Substituir valores maiores que 15 por 15 na coluna 'team1_corners'\n",
    "datatotal.loc[datatotal['team1_corners'] > 15, 'team1_corners'] = 15\n",
    "\n",
    "# Substituir valores maiores que 15 por 15 na coluna 'team2_corners'\n",
    "datatotal.loc[datatotal['team2_corners'] > 15, 'team2_corners'] = 15\n",
    "\n",
    "\n",
    "datatotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ITA A' 'ING B' 'ING A' 'ING C' 'ESC A' 'ING D' 'BRA A' 'SUE A' 'POR A'\n",
      " 'ESP A' 'FRA A' 'HOL A' 'ALE A' 'TUR A' 'BEL A' 'ALE B' 'ESP B' 'ITA B'\n",
      " 'FRA B' 'GRE A']\n"
     ]
    }
   ],
   "source": [
    "champ_uniques = datatotal['championship'].unique()\n",
    "print(champ_uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359056, 2)\n",
      "(359056, 2)\n",
      "(359056, 101)\n",
      "Total number of rows with 'NaN' or an empty value: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>championship</th>\n",
       "      <th>team1</th>\n",
       "      <th>team2</th>\n",
       "      <th>team1_red_cards</th>\n",
       "      <th>team2_red_cards</th>\n",
       "      <th>team1_big_win</th>\n",
       "      <th>team1_big_loss</th>\n",
       "      <th>team2_big_win</th>\n",
       "      <th>team2_big_loss</th>\n",
       "      <th>team1_big_wins_last5</th>\n",
       "      <th>...</th>\n",
       "      <th>team1_undefeated_streak</th>\n",
       "      <th>team1_winning_streak</th>\n",
       "      <th>team1_without_winning_streak</th>\n",
       "      <th>team2_losing_streak</th>\n",
       "      <th>team2_strength</th>\n",
       "      <th>team2_undefeated_streak</th>\n",
       "      <th>team2_winning_streak</th>\n",
       "      <th>team2_without_winning_streak</th>\n",
       "      <th>team1_suspended_players</th>\n",
       "      <th>team2_suspended_players</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>359051</th>\n",
       "      <td>BRA A</td>\n",
       "      <td>vasco da gama</td>\n",
       "      <td>atlético mineiro</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.908231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>-0.485300</td>\n",
       "      <td>-0.385256</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>0.903984</td>\n",
       "      <td>-0.055780</td>\n",
       "      <td>1.006009</td>\n",
       "      <td>-0.691875</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359052</th>\n",
       "      <td>BRA A</td>\n",
       "      <td>coritiba</td>\n",
       "      <td>flamengo</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>0.183852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>-0.485300</td>\n",
       "      <td>0.353249</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>0.930439</td>\n",
       "      <td>-0.359327</td>\n",
       "      <td>-0.568033</td>\n",
       "      <td>0.059214</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359053</th>\n",
       "      <td>BRA A</td>\n",
       "      <td>bahia</td>\n",
       "      <td>red bull bragantino</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>1.887170</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>1.887170</td>\n",
       "      <td>-0.908231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>-0.485300</td>\n",
       "      <td>1.830258</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>1.568237</td>\n",
       "      <td>0.551315</td>\n",
       "      <td>-0.568033</td>\n",
       "      <td>-0.316330</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359054</th>\n",
       "      <td>BRA A</td>\n",
       "      <td>santos</td>\n",
       "      <td>grêmio</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>2.583614</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.908231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>-0.485300</td>\n",
       "      <td>1.091753</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>0.472811</td>\n",
       "      <td>-0.359327</td>\n",
       "      <td>0.218988</td>\n",
       "      <td>-0.691875</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359055</th>\n",
       "      <td>BRA A</td>\n",
       "      <td>goiás</td>\n",
       "      <td>athletico-pr</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.908231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929595</td>\n",
       "      <td>1.134265</td>\n",
       "      <td>-0.754509</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>0.789664</td>\n",
       "      <td>0.854862</td>\n",
       "      <td>0.218988</td>\n",
       "      <td>-0.691875</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       championship          team1                team2  team1_red_cards  \\\n",
       "359051        BRA A  vasco da gama     atlético mineiro        -0.284925   \n",
       "359052        BRA A       coritiba             flamengo        -0.284925   \n",
       "359053        BRA A          bahia  red bull bragantino        -0.284925   \n",
       "359054        BRA A         santos               grêmio        -0.284925   \n",
       "359055        BRA A          goiás         athletico-pr        -0.284925   \n",
       "\n",
       "        team2_red_cards  team1_big_win  team1_big_loss  team2_big_win  \\\n",
       "359051        -0.335132      -0.529894       -0.375999      -0.375999   \n",
       "359052        -0.335132      -0.529894       -0.375999      -0.375999   \n",
       "359053        -0.335132       1.887170       -0.375999      -0.375999   \n",
       "359054         2.583614      -0.529894       -0.375999      -0.375999   \n",
       "359055        -0.335132      -0.529894       -0.375999      -0.375999   \n",
       "\n",
       "        team2_big_loss  team1_big_wins_last5  ...  team1_undefeated_streak  \\\n",
       "359051       -0.529894             -0.908231  ...                 0.004589   \n",
       "359052       -0.529894              0.183852  ...                -0.612081   \n",
       "359053        1.887170             -0.908231  ...                -0.612081   \n",
       "359054       -0.529894             -0.908231  ...                -0.612081   \n",
       "359055       -0.529894             -0.908231  ...                 0.929595   \n",
       "\n",
       "        team1_winning_streak  team1_without_winning_streak  \\\n",
       "359051             -0.485300                     -0.385256   \n",
       "359052             -0.485300                      0.353249   \n",
       "359053             -0.485300                      1.830258   \n",
       "359054             -0.485300                      1.091753   \n",
       "359055              1.134265                     -0.754509   \n",
       "\n",
       "        team2_losing_streak  team2_strength  team2_undefeated_streak  \\\n",
       "359051            -0.531362        0.903984                -0.055780   \n",
       "359052            -0.531362        0.930439                -0.359327   \n",
       "359053            -0.531362        1.568237                 0.551315   \n",
       "359054            -0.531362        0.472811                -0.359327   \n",
       "359055            -0.531362        0.789664                 0.854862   \n",
       "\n",
       "        team2_winning_streak  team2_without_winning_streak  \\\n",
       "359051              1.006009                     -0.691875   \n",
       "359052             -0.568033                      0.059214   \n",
       "359053             -0.568033                     -0.316330   \n",
       "359054              0.218988                     -0.691875   \n",
       "359055              0.218988                     -0.691875   \n",
       "\n",
       "        team1_suspended_players  team2_suspended_players  \n",
       "359051                -0.324714                -0.298226  \n",
       "359052                -0.324714                -0.298226  \n",
       "359053                -0.324714                -0.298226  \n",
       "359054                -0.324714                -0.298226  \n",
       "359055                -0.324714                -0.298226  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate the target variables\n",
    "y1 = datatotal[['team1_corners','championship']]\n",
    "y2 = datatotal[['team2_corners','championship']]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "datatotal_copy = datatotal.drop(['is_future_match',\n",
    "                                 'season', 'date', 'team1_goals', 'team2_goals','goal_diff_team1','goal_diff_team2', 'team1_corners', 'team2_corners', 'team1_fouls', 'team2_fouls',\n",
    "                                'team1_shots_on_target', 'team1_total_shots', 'team1_shots_out', 'team2_shots_on_target', 'team2_total_shots', 'team2_shots_out',\n",
    "                                'corners_diff_team1', 'corners_diff_team2','team1_ah-2.5_win','team1_ah-2.5_loss','team2_ah-2.5_win','team2_ah-2.5_loss','team1_ah+2.5_win','team1_ah+2.5_loss','team2_ah+2.5_win','team2_ah+2.5_loss','team1_over3.5','team1_under3.5','team2_over3.5','team2_under3.5','team1_over4.5','team1_under4.5','team2_over4.5','team2_under4.5','team1_over6.5','team1_under6.5','team2_over6.5','team2_under6.5'], axis=1)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Identify the columns to scale, excluding 'team1' and 'team2'\n",
    "columns_to_scale = datatotal_copy.drop(['championship','team1', 'team2'], axis=1).columns\n",
    "\n",
    "# Fit the scaler using the data\n",
    "scaler.fit(datatotal_copy[columns_to_scale])\n",
    "\n",
    "# Transform the data\n",
    "dataset_scaled = scaler.transform(datatotal_copy[columns_to_scale])\n",
    "\n",
    "# Create a new DataFrame with the scaled data\n",
    "dataset_scaled = pd.DataFrame(dataset_scaled, columns=columns_to_scale).reset_index(drop=True)\n",
    "\n",
    "# Add back the 'team1' and 'team2' columns\n",
    "dataset_scaled = pd.concat([datatotal[['championship','team1', 'team2']].reset_index(drop=True), dataset_scaled], axis=1)\n",
    "\n",
    "# Scale the future_matches_filtred DataFrame\n",
    "future_matches_filtred_scaled = scaler.transform(future_matches[columns_to_scale])\n",
    "future_matches_filtred_scaled = pd.DataFrame(future_matches_filtred_scaled, columns=columns_to_scale).reset_index(drop=True)\n",
    "future_matches_filtred_scaled = pd.concat([future_matches[['championship','team1', 'team2']].reset_index(drop=True), future_matches_filtred_scaled], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "print(y1.shape)\n",
    "print(y2.shape)#print(future_matches_filtred_scaled.shape)\n",
    "print(dataset_scaled.shape)\n",
    "import numpy as np\n",
    "\n",
    "counter = 0  # Initialize counter\n",
    "for index, row in dataset_scaled.iterrows():\n",
    "    if row.isnull().any() or row.eq('').any():\n",
    "        #print(f\"Row {index} contains 'NaN' or an empty value.\")\n",
    "        counter += 1  # Increase counter if condition is met\n",
    "\n",
    "print(f\"Total number of rows with 'NaN' or an empty value: {counter}\")\n",
    "\n",
    "dataset_scaled[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 99)\n",
      "(359056, 99)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>championship</th>\n",
       "      <th>team1</th>\n",
       "      <th>team2</th>\n",
       "      <th>team1_red_cards</th>\n",
       "      <th>team2_red_cards</th>\n",
       "      <th>team1_big_win</th>\n",
       "      <th>team1_big_loss</th>\n",
       "      <th>team2_big_win</th>\n",
       "      <th>team2_big_loss</th>\n",
       "      <th>team1_big_wins_last5</th>\n",
       "      <th>...</th>\n",
       "      <th>team1_undefeated_streak</th>\n",
       "      <th>team1_winning_streak</th>\n",
       "      <th>team1_without_winning_streak</th>\n",
       "      <th>team2_losing_streak</th>\n",
       "      <th>team2_strength</th>\n",
       "      <th>team2_undefeated_streak</th>\n",
       "      <th>team2_winning_streak</th>\n",
       "      <th>team2_without_winning_streak</th>\n",
       "      <th>team1_suspended_players</th>\n",
       "      <th>team2_suspended_players</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ITA A</td>\n",
       "      <td>240</td>\n",
       "      <td>266</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>2.368017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.303746</td>\n",
       "      <td>0.324483</td>\n",
       "      <td>-0.754509</td>\n",
       "      <td>0.410154</td>\n",
       "      <td>0.783051</td>\n",
       "      <td>-0.662874</td>\n",
       "      <td>-0.568033</td>\n",
       "      <td>-0.316330</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ING B</td>\n",
       "      <td>240</td>\n",
       "      <td>266</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>2.368017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.303746</td>\n",
       "      <td>0.324483</td>\n",
       "      <td>-0.754509</td>\n",
       "      <td>0.410154</td>\n",
       "      <td>0.783051</td>\n",
       "      <td>-0.662874</td>\n",
       "      <td>-0.568033</td>\n",
       "      <td>-0.316330</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ING B</td>\n",
       "      <td>451</td>\n",
       "      <td>387</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>2.583614</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.908231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>1.134265</td>\n",
       "      <td>-0.754509</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>-2.193350</td>\n",
       "      <td>-0.359327</td>\n",
       "      <td>0.218988</td>\n",
       "      <td>-0.691875</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ING A</td>\n",
       "      <td>101</td>\n",
       "      <td>31</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>2.659585</td>\n",
       "      <td>2.659585</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>0.183852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>-0.485300</td>\n",
       "      <td>-0.385256</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>2.276209</td>\n",
       "      <td>0.854862</td>\n",
       "      <td>0.218988</td>\n",
       "      <td>-0.691875</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ING A</td>\n",
       "      <td>147</td>\n",
       "      <td>273</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.908231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>-0.485300</td>\n",
       "      <td>0.353249</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>2.271252</td>\n",
       "      <td>-0.359327</td>\n",
       "      <td>0.218988</td>\n",
       "      <td>-0.691875</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359051</th>\n",
       "      <td>BRA A</td>\n",
       "      <td>502</td>\n",
       "      <td>470</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.908231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>-0.485300</td>\n",
       "      <td>-0.385256</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>0.903984</td>\n",
       "      <td>-0.055780</td>\n",
       "      <td>1.006009</td>\n",
       "      <td>-0.691875</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359052</th>\n",
       "      <td>BRA A</td>\n",
       "      <td>476</td>\n",
       "      <td>482</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>0.183852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>-0.485300</td>\n",
       "      <td>0.353249</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>0.930439</td>\n",
       "      <td>-0.359327</td>\n",
       "      <td>-0.568033</td>\n",
       "      <td>0.059214</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359053</th>\n",
       "      <td>BRA A</td>\n",
       "      <td>471</td>\n",
       "      <td>498</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>1.887170</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>1.887170</td>\n",
       "      <td>-0.908231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>-0.485300</td>\n",
       "      <td>1.830258</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>1.568237</td>\n",
       "      <td>0.551315</td>\n",
       "      <td>-0.568033</td>\n",
       "      <td>-0.316330</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359054</th>\n",
       "      <td>BRA A</td>\n",
       "      <td>499</td>\n",
       "      <td>486</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>2.583614</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.908231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>-0.485300</td>\n",
       "      <td>1.091753</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>0.472811</td>\n",
       "      <td>-0.359327</td>\n",
       "      <td>0.218988</td>\n",
       "      <td>-0.691875</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359055</th>\n",
       "      <td>BRA A</td>\n",
       "      <td>485</td>\n",
       "      <td>469</td>\n",
       "      <td>-0.284925</td>\n",
       "      <td>-0.335132</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.529894</td>\n",
       "      <td>-0.908231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929595</td>\n",
       "      <td>1.134265</td>\n",
       "      <td>-0.754509</td>\n",
       "      <td>-0.531362</td>\n",
       "      <td>0.789664</td>\n",
       "      <td>0.854862</td>\n",
       "      <td>0.218988</td>\n",
       "      <td>-0.691875</td>\n",
       "      <td>-0.324714</td>\n",
       "      <td>-0.298226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359056 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       championship  team1  team2  team1_red_cards  team2_red_cards  \\\n",
       "0             ITA A    240    266        -0.284925        -0.335132   \n",
       "1             ING B    240    266        -0.284925        -0.335132   \n",
       "2             ING B    451    387        -0.284925         2.583614   \n",
       "3             ING A    101     31        -0.284925        -0.335132   \n",
       "4             ING A    147    273        -0.284925        -0.335132   \n",
       "...             ...    ...    ...              ...              ...   \n",
       "359051        BRA A    502    470        -0.284925        -0.335132   \n",
       "359052        BRA A    476    482        -0.284925        -0.335132   \n",
       "359053        BRA A    471    498        -0.284925        -0.335132   \n",
       "359054        BRA A    499    486        -0.284925         2.583614   \n",
       "359055        BRA A    485    469        -0.284925        -0.335132   \n",
       "\n",
       "        team1_big_win  team1_big_loss  team2_big_win  team2_big_loss  \\\n",
       "0           -0.529894       -0.375999      -0.375999       -0.529894   \n",
       "1           -0.529894       -0.375999      -0.375999       -0.529894   \n",
       "2           -0.529894       -0.375999      -0.375999       -0.529894   \n",
       "3           -0.529894        2.659585       2.659585       -0.529894   \n",
       "4           -0.529894       -0.375999      -0.375999       -0.529894   \n",
       "...               ...             ...            ...             ...   \n",
       "359051      -0.529894       -0.375999      -0.375999       -0.529894   \n",
       "359052      -0.529894       -0.375999      -0.375999       -0.529894   \n",
       "359053       1.887170       -0.375999      -0.375999        1.887170   \n",
       "359054      -0.529894       -0.375999      -0.375999       -0.529894   \n",
       "359055      -0.529894       -0.375999      -0.375999       -0.529894   \n",
       "\n",
       "        team1_big_wins_last5  ...  team1_undefeated_streak  \\\n",
       "0                   2.368017  ...                -0.303746   \n",
       "1                   2.368017  ...                -0.303746   \n",
       "2                  -0.908231  ...                 0.004589   \n",
       "3                   0.183852  ...                -0.612081   \n",
       "4                  -0.908231  ...                -0.612081   \n",
       "...                      ...  ...                      ...   \n",
       "359051             -0.908231  ...                 0.004589   \n",
       "359052              0.183852  ...                -0.612081   \n",
       "359053             -0.908231  ...                -0.612081   \n",
       "359054             -0.908231  ...                -0.612081   \n",
       "359055             -0.908231  ...                 0.929595   \n",
       "\n",
       "        team1_winning_streak  team1_without_winning_streak  \\\n",
       "0                   0.324483                     -0.754509   \n",
       "1                   0.324483                     -0.754509   \n",
       "2                   1.134265                     -0.754509   \n",
       "3                  -0.485300                     -0.385256   \n",
       "4                  -0.485300                      0.353249   \n",
       "...                      ...                           ...   \n",
       "359051             -0.485300                     -0.385256   \n",
       "359052             -0.485300                      0.353249   \n",
       "359053             -0.485300                      1.830258   \n",
       "359054             -0.485300                      1.091753   \n",
       "359055              1.134265                     -0.754509   \n",
       "\n",
       "        team2_losing_streak  team2_strength  team2_undefeated_streak  \\\n",
       "0                  0.410154        0.783051                -0.662874   \n",
       "1                  0.410154        0.783051                -0.662874   \n",
       "2                 -0.531362       -2.193350                -0.359327   \n",
       "3                 -0.531362        2.276209                 0.854862   \n",
       "4                 -0.531362        2.271252                -0.359327   \n",
       "...                     ...             ...                      ...   \n",
       "359051            -0.531362        0.903984                -0.055780   \n",
       "359052            -0.531362        0.930439                -0.359327   \n",
       "359053            -0.531362        1.568237                 0.551315   \n",
       "359054            -0.531362        0.472811                -0.359327   \n",
       "359055            -0.531362        0.789664                 0.854862   \n",
       "\n",
       "        team2_winning_streak  team2_without_winning_streak  \\\n",
       "0                  -0.568033                     -0.316330   \n",
       "1                  -0.568033                     -0.316330   \n",
       "2                   0.218988                     -0.691875   \n",
       "3                   0.218988                     -0.691875   \n",
       "4                   0.218988                     -0.691875   \n",
       "...                      ...                           ...   \n",
       "359051              1.006009                     -0.691875   \n",
       "359052             -0.568033                      0.059214   \n",
       "359053             -0.568033                     -0.316330   \n",
       "359054              0.218988                     -0.691875   \n",
       "359055              0.218988                     -0.691875   \n",
       "\n",
       "        team1_suspended_players  team2_suspended_players  \n",
       "0                     -0.324714                -0.298226  \n",
       "1                     -0.324714                -0.298226  \n",
       "2                     -0.324714                -0.298226  \n",
       "3                     -0.324714                -0.298226  \n",
       "4                     -0.324714                -0.298226  \n",
       "...                         ...                      ...  \n",
       "359051                -0.324714                -0.298226  \n",
       "359052                -0.324714                -0.298226  \n",
       "359053                -0.324714                -0.298226  \n",
       "359054                -0.324714                -0.298226  \n",
       "359055                -0.324714                -0.298226  \n",
       "\n",
       "[359056 rows x 99 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Preprocess the dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label encode the team names\n",
    "teams = pd.concat([dataset_scaled['team1'], dataset_scaled['team2']]).astype(str)\n",
    "le = LabelEncoder().fit(teams)\n",
    "\n",
    "dataset_scaled['team1'] = le.transform(dataset_scaled['team1'].astype(str))\n",
    "dataset_scaled['team2'] = le.transform(dataset_scaled['team2'].astype(str))\n",
    "\n",
    "# Encode the team names in the future_matches_filtred dataset\n",
    "future_matches_filtred_scaled['team1'] = le.transform(future_matches_filtred_scaled['team1'].astype(str))\n",
    "future_matches_filtred_scaled['team2'] = le.transform(future_matches_filtred_scaled['team2'].astype(str))\n",
    "\n",
    "final_df = dataset_scaled.drop(['result_team1', 'result_team2'], axis=1)\n",
    "future_matches_filtred_scaled = future_matches_filtred_scaled.drop(['result_team1', 'result_team2'], axis=1)\n",
    "\n",
    "print(future_matches_filtred_scaled.shape)\n",
    "print(final_df.shape)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basta rodar até aqui, agora só ir carregar os pickles e receber a output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basta rodar até aqui, agora só ir carregar os pickles e receber a output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basta rodar até aqui, agora só ir carregar os pickles e receber a output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quantidade de np.nan em linhas eram 0\n"
     ]
    }
   ],
   "source": [
    "#final_df.replace('', np.nan, inplace=True)\n",
    "print(f\"A quantidade de np.nan em linhas eram {final_df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ITA A', 'ING B', 'ING A', 'ING C', 'ESC A', 'ING D', 'BRA A',\n",
       "       'SUE A', 'POR A', 'ESP A', 'FRA A', 'HOL A', 'ALE A', 'TUR A',\n",
       "       'BEL A', 'ALE B', 'ESP B', 'ITA B', 'FRA B', 'GRE A'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['championship'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando X e Y para cada campeonato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359056, 99)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359056, 99)\n",
      "(359056, 2)\n",
      "(359056, 2)\n",
      "(359056, 2)\n",
      "(359056, 2)\n",
      "Shape of X for each championship:\n",
      "ALE A: (8740, 98)\n",
      "ALE B: (8740, 98)\n",
      "BEL A: (7515, 98)\n",
      "BRA A: (20753, 98)\n",
      "ESC A: (30493, 98)\n",
      "ESP A: (11360, 98)\n",
      "ESP B: (7681, 98)\n",
      "FRA A: (17786, 98)\n",
      "FRA B: (3425, 98)\n",
      "GRE A: (1470, 98)\n",
      "HOL A: (5480, 98)\n",
      "ING A: (29348, 98)\n",
      "ING B: (42704, 98)\n",
      "ING C: (36889, 98)\n",
      "ING D: (30493, 98)\n",
      "ITA A: (23675, 98)\n",
      "ITA B: (7684, 98)\n",
      "POR A: (37894, 98)\n",
      "SUE A: (20716, 98)\n",
      "TUR A: (6210, 98)\n",
      "\n",
      "Shape of y1 for each championship:\n",
      "ALE A: (8740, 1)\n",
      "ALE B: (8740, 1)\n",
      "BEL A: (7515, 1)\n",
      "BRA A: (20753, 1)\n",
      "ESC A: (30493, 1)\n",
      "ESP A: (11360, 1)\n",
      "ESP B: (7681, 1)\n",
      "FRA A: (17786, 1)\n",
      "FRA B: (3425, 1)\n",
      "GRE A: (1470, 1)\n",
      "HOL A: (5480, 1)\n",
      "ING A: (29348, 1)\n",
      "ING B: (42704, 1)\n",
      "ING C: (36889, 1)\n",
      "ING D: (30493, 1)\n",
      "ITA A: (23675, 1)\n",
      "ITA B: (7684, 1)\n",
      "POR A: (37894, 1)\n",
      "SUE A: (20716, 1)\n",
      "TUR A: (6210, 1)\n",
      "\n",
      "Shape of y2 for each championship:\n",
      "ALE A: (8740, 1)\n",
      "ALE B: (8740, 1)\n",
      "BEL A: (7515, 1)\n",
      "BRA A: (20753, 1)\n",
      "ESC A: (30493, 1)\n",
      "ESP A: (11360, 1)\n",
      "ESP B: (7681, 1)\n",
      "FRA A: (17786, 1)\n",
      "FRA B: (3425, 1)\n",
      "GRE A: (1470, 1)\n",
      "HOL A: (5480, 1)\n",
      "ING A: (29348, 1)\n",
      "ING B: (42704, 1)\n",
      "ING C: (36889, 1)\n",
      "ING D: (30493, 1)\n",
      "ITA A: (23675, 1)\n",
      "ITA B: (7684, 1)\n",
      "POR A: (37894, 1)\n",
      "SUE A: (20716, 1)\n",
      "TUR A: (6210, 1)\n"
     ]
    }
   ],
   "source": [
    "# Para X\n",
    "print(final_df.shape)\n",
    "final_df = final_df.dropna()\n",
    "print(final_df.shape)\n",
    "\n",
    "grouped_X = final_df.groupby('championship')\n",
    "\n",
    "dfs_X = {name: group.copy() for name, group in grouped_X}\n",
    "\n",
    "# Para Y1 e Y2\n",
    "print(y1.shape)\n",
    "print(y2.shape)\n",
    "\n",
    "y1 = y1.loc[final_df.index]\n",
    "y2 = y2.loc[final_df.index]\n",
    "\n",
    "print(y1.shape)\n",
    "print(y2.shape)\n",
    "\n",
    "grouped_y1 = y1.groupby('championship')\n",
    "grouped_y2 = y2.groupby('championship')\n",
    "\n",
    "dfs_y1 = {name: group.copy() for name, group in grouped_y1}\n",
    "dfs_y2 = {name: group.copy() for name, group in grouped_y2}\n",
    "\n",
    "# Agora você tem três dicionários: dfs_X, dfs_y1, dfs_y2\n",
    "# Cada um com chaves sendo 'championship' e valores sendo os DataFrames correspondentes\n",
    "\n",
    "# Calcule a média de 'team1_corners' e 'team2_corners' para cada campeonato\n",
    "mean_y1 = {championship: df['team1_corners'].mean() for championship, df in dfs_y1.items()}\n",
    "mean_y2 = {championship: df['team2_corners'].mean() for championship, df in dfs_y2.items()}\n",
    "\n",
    "# Removendo a coluna 'championship' de todos os dicionários\n",
    "for df_dict in [dfs_X, dfs_y1, dfs_y2]:\n",
    "    for df in df_dict.values():\n",
    "        if 'championship' in df.columns:\n",
    "            df.drop('championship', axis=1, inplace=True)\n",
    "\n",
    "# Para dfs_X\n",
    "print(\"Shape of X for each championship:\")\n",
    "for championship, df in dfs_X.items():\n",
    "    print(f\"{championship}: {df.shape}\")\n",
    "\n",
    "# Para dfs_y1\n",
    "print(\"\\nShape of y1 for each championship:\")\n",
    "for championship, df in dfs_y1.items():\n",
    "    print(f\"{championship}: {df.shape}\")\n",
    "\n",
    "# Para dfs_y2\n",
    "print(\"\\nShape of y2 for each championship:\")\n",
    "for championship, df in dfs_y2.items():\n",
    "    print(f\"{championship}: {df.shape}\")            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, -1.0647107369924285, 1.3636363636363635, <tf.Tensor: shape=(), dtype=float64, numpy=4.298925626643935>)\n",
      "(2, -1.0647107369924285, 0.6818181818181818, <tf.Tensor: shape=(), dtype=float64, numpy=1.6171074448257534>)\n",
      "(2, -0.6418538861723949, 0.8823529411764705, <tf.Tensor: shape=(), dtype=float64, numpy=2.2404990550040758>)\n",
      "(2, -1.1314021114911004, 0.6521739130434783, <tf.Tensor: shape=(), dtype=float64, numpy=1.520771801552378>)\n",
      "(2.0, -0.47000362924573535, 0.9677419354838711, <tf.Tensor: shape=(), dtype=float64, numpy=2.4977383062381358>)\n",
      "(2.0, -0.47000362924573535, 0.9677419354838711, <tf.Tensor: shape=(), dtype=float64, numpy=2.4977383062381358>)\n",
      "(0.5, -1.5260563034950492, 0.12295081967213116, <tf.Tensor: shape=(), dtype=float32, numpy=1e-04>)\n",
      "(0.7999999999999998, -1.3609765531356006, 0.22222222222222218, <tf.Tensor: shape=(), dtype=float32, numpy=1e-04>)\n",
      "(0.7999999999999998, -0.8329091229351039, 0.31578947368421045, <tf.Tensor: shape=(), dtype=float64, numpy=0.2828803507491064>)\n",
      "(1, -1.62924053973028, 0.2272727272727273, <tf.Tensor: shape=(), dtype=float32, numpy=1e-04>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def relu(x):\n",
    "    return max(0.0001, x)\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    avg = 6.6\n",
    "    abs_error = relu(np.abs(y_true - y_pred))\n",
    "    distance_to_avg = relu(np.abs(y_pred - avg))\n",
    "    reward = np.log(distance_to_avg + 1.5)\n",
    "    penalty =relu( 1.5 * abs_error / (distance_to_avg + 3)) \n",
    "    custom_loss_value = relu(abs_error - reward + penalty)\n",
    "    return (abs_error), -reward, penalty, tf.reduce_mean(custom_loss_value)\n",
    "\n",
    "print(custom_loss(y_true=4,y_pred=8))\n",
    "print(custom_loss(y_true=6,y_pred=8))\n",
    "print(custom_loss(y_true=5,y_pred=7))\n",
    "print(custom_loss(y_true=7,y_pred=5))\n",
    "print(custom_loss(y_true=8.5,y_pred=6.5))\n",
    "print(custom_loss(y_true=4.5,y_pred=6.5))\n",
    "print(custom_loss(y_true=4,y_pred=3.5))\n",
    "print(custom_loss(y_true=5,y_pred=4.2))\n",
    "print(custom_loss(y_true=5,y_pred=5.8))\n",
    "print(custom_loss(y_true=4,y_pred=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The list/tuple elements must be unique strings of predefined scorers.  One or more of the elements were callables. Use a dict of score name mapped to the scorer callable. Got [make_scorer(custom_loss, greater_is_better=False)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 53\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m best_params, best_score\n\u001b[0;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dfs_X\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     52\u001b[0m     \u001b[39m# Perform grid search for Lasso models for both teams\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     best_lasso_params1, best_score1 \u001b[39m=\u001b[39m perform_grid_search(Lasso(max_iter\u001b[39m=\u001b[39;49m\u001b[39m10000000\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m), lasso_param_grid, dfs_X[key], dfs_y1[key])\n\u001b[0;32m     54\u001b[0m     best_lasso_params2, best_score2 \u001b[39m=\u001b[39m perform_grid_search(Lasso(max_iter\u001b[39m=\u001b[39m\u001b[39m10000000\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m), lasso_param_grid, dfs_X[key], dfs_y2[key])\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Fit the model with the best parameters and compute MAE\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[99], line 46\u001b[0m, in \u001b[0;36mperform_grid_search\u001b[1;34m(model, param_grid, X_train, y_train)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_grid_search\u001b[39m(model, param_grid, X_train, y_train):\n\u001b[0;32m     45\u001b[0m     grid_search \u001b[39m=\u001b[39m GridSearchCV(model, param_grid, scoring\u001b[39m=\u001b[39m [custom_scorer], cv\u001b[39m=\u001b[39mtscv, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m     grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     47\u001b[0m     best_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n\u001b[0;32m     48\u001b[0m     best_score \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m grid_search\u001b[39m.\u001b[39mbest_score_  \u001b[39m# Switching back to positive MSE\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:778\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    776\u001b[0m     scorers \u001b[39m=\u001b[39m check_scoring(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring)\n\u001b[0;32m    777\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 778\u001b[0m     scorers \u001b[39m=\u001b[39m _check_multimetric_scoring(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscoring)\n\u001b[0;32m    779\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_refit_for_multimetric(scorers)\n\u001b[0;32m    780\u001b[0m     refit_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefit\n",
      "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:569\u001b[0m, in \u001b[0;36m_check_multimetric_scoring\u001b[1;34m(estimator, scoring)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(k, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m keys):\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mcallable\u001b[39m(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m keys):\n\u001b[1;32m--> 569\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00merr_msg\u001b[39m}\u001b[39;00m\u001b[39m One or more of the elements \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwere callables. Use a dict of score \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mname mapped to the scorer callable. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot \u001b[39m\u001b[39m{\u001b[39;00mscoring\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m         )\n\u001b[0;32m    575\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    576\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    577\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00merr_msg\u001b[39m}\u001b[39;00m\u001b[39m Non-string types were found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the given list. Got \u001b[39m\u001b[39m{\u001b[39;00mscoring\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The list/tuple elements must be unique strings of predefined scorers.  One or more of the elements were callables. Use a dict of score name mapped to the scorer callable. Got [make_scorer(custom_loss, greater_is_better=False)]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "import tensorflow as tf\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x))\n",
    "\n",
    "def tanh_to_01(x):\n",
    "    return (np.tanh(x) + 1) / 2\n",
    "\n",
    "def relu(x):\n",
    "    return max(0.0001, x)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "\n",
    "    avg = np.mean(y_true)\n",
    "    abs_error = np.abs(y_true - y_pred)\n",
    "    distance_to_avg = np.abs(y_pred - avg)\n",
    "    reward = np.log(distance_to_avg + 1.5)\n",
    "    penalty = 1.5 * abs_error / (distance_to_avg + 3) \n",
    "    custom_loss_value = relu(abs_error - reward + penalty)\n",
    "    return np.mean(custom_loss_value)\n",
    "\n",
    "custom_scorer = make_scorer(custom_loss, greater_is_better=False)\n",
    "\n",
    "\n",
    "# Dicionário para armazenar os modelos Lasso\n",
    "models_lasso = {}\n",
    "\n",
    "# Hyperparameter grid for Lasso\n",
    "lasso_param_grid = {\n",
    "    'alpha': np.logspace(-5, -2.5, num=100)\n",
    "}\n",
    "\n",
    "# Use 5 splits\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "def perform_grid_search(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring= [custom_scorer], cv=tscv, verbose=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = -1 * grid_search.best_score_  # Switching back to positive MSE\n",
    "    return best_params, best_score\n",
    "\n",
    "for key in dfs_X.keys():\n",
    "    # Perform grid search for Lasso models for both teams\n",
    "    best_lasso_params1, best_score1 = perform_grid_search(Lasso(max_iter=10000000, random_state=42), lasso_param_grid, dfs_X[key], dfs_y1[key])\n",
    "    best_lasso_params2, best_score2 = perform_grid_search(Lasso(max_iter=10000000, random_state=42), lasso_param_grid, dfs_X[key], dfs_y2[key])\n",
    "    \n",
    "    # Fit the model with the best parameters and compute MAE\n",
    "    best_model1 = Lasso(alpha=best_lasso_params1['alpha'], max_iter=10000000, random_state=42)\n",
    "    best_model1.fit(dfs_X[key], dfs_y1[key])\n",
    "    min_mae1 = mean_absolute_error(dfs_y1[key], best_model1.predict(dfs_X[key]))\n",
    "    \n",
    "    best_model2 = Lasso(alpha=best_lasso_params2['alpha'], max_iter=10000000, random_state=42)\n",
    "    best_model2.fit(dfs_X[key], dfs_y2[key])\n",
    "    min_mae2 = mean_absolute_error(dfs_y2[key], best_model2.predict(dfs_X[key]))\n",
    "\n",
    "    # Adicionar os modelos treinados ao dicionário\n",
    "    models_lasso[key] = {\"model1\": best_model1, \"model2\": best_model2}\n",
    "\n",
    "    # Calculate relative errors\n",
    "    erro_rel_1 = min_mae1 / mean_y1[key]\n",
    "    erro_rel_2 = min_mae2 / mean_y2[key]\n",
    "    tot_erro_rel = erro_rel_2 + erro_rel_1\n",
    "\n",
    "    print(key)\n",
    "    print(f\"\\nBest parameters for Lasso (team1): {best_lasso_params1}\")\n",
    "    print(f\"Best parameters for Lasso (team2): {best_lasso_params2}\\n\")\n",
    "    print(f\"Best MSE for Lasso (team1): {best_score1}\")\n",
    "    print(f\"Best MSE for Lasso (team2): {best_score2}\\n\")\n",
    "    print(f\"Minimum MAE for Lasso (team1): {min_mae1}\")\n",
    "    print(f\"Erro relativo a Y_1 : {erro_rel_1}\")\n",
    "    print(f\"Minimum MAE for Lasso (team2): {min_mae2}\")\n",
    "    print(f\"Erro relativo a Y_2 : {erro_rel_2}\")\n",
    "    print(f\"Erro relativo da COMPETIÇÃO {key} : {tot_erro_rel}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o treinamento dos modelos em pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Suponhamos que model1_lasso e model2_lasso são seus modelos treinados\n",
    "for key in models_lasso.keys():\n",
    "    with open(f'model1_lasso_{key}.pkl', 'wb') as f:\n",
    "        pickle.dump(models_lasso[key][\"model1\"], f)\n",
    "        \n",
    "    with open(f'model2_lasso_{key}.pkl', 'wb') as f:\n",
    "        pickle.dump(models_lasso[key][\"model2\"], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"salvo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for key, models in models_lasso.items():\n",
    "    model1_lasso = models[\"model1\"]\n",
    "    model2_lasso = models[\"model2\"]\n",
    "    \n",
    "    # Coeficientes para model1_lasso\n",
    "    coef_model1 = pd.Series(model1_lasso.coef_, index=dfs_X[key].columns)\n",
    "    print(f\"Coeficientes para {key} - model1_lasso\")\n",
    "    print(coef_model1.sort_values(ascending=False))  # Ordenando para melhor visualização\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Coeficientes para model2_lasso\n",
    "    coef_model2 = pd.Series(model2_lasso.coef_, index=dfs_X[key].columns)\n",
    "    print(f\"Coeficientes para {key} - model2_lasso\")\n",
    "    print(coef_model2.sort_values(ascending=False))  # Ordenando para melhor visualização\n",
    "    print(\"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando os modelos Lassos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Lista todos os arquivos na pasta atual\n",
    "files_in_directory = os.listdir()\n",
    "\n",
    "# Filtra somente os arquivos .pkl\n",
    "pkl_files = [f for f in files_in_directory if f.endswith('.pkl')]\n",
    "\n",
    "# Dicionário para armazenar os modelos Lasso carregados\n",
    "loaded_models_lasso = {}\n",
    "\n",
    "# Loop para carregar os modelos\n",
    "for file in pkl_files:\n",
    "    # Verifica se o arquivo é um dos arquivos de modelo desejados\n",
    "    if 'model1_lasso_' in file or 'model2_lasso_' in file:\n",
    "        with open(file, 'rb') as f:\n",
    "            model_loaded = pickle.load(f)\n",
    "        \n",
    "        # Extrai a chave e o nome do modelo a partir do nome do arquivo\n",
    "        # Assume que os arquivos têm nomes como 'model1_lasso_BRA A.pkl'\n",
    "        parts = file.split('_')\n",
    "        model_name = parts[0]\n",
    "        key = ' '.join(parts[2:]).replace('.pkl', '')\n",
    "        \n",
    "        # Insere o modelo carregado no dicionário\n",
    "        if key not in loaded_models_lasso:\n",
    "            loaded_models_lasso[key] = {}\n",
    "        \n",
    "        loaded_models_lasso[key][model_name] = model_loaded\n",
    "\n",
    "# Agora, loaded_models_lasso é um dicionário com os modelos Lasso carregados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos carregados:\n",
      "ALE A\n",
      "ALE B\n",
      "BEL A\n",
      "BRA A\n",
      "ESC A\n",
      "ESP A\n",
      "ESP B\n",
      "FRA A\n",
      "FRA B\n",
      "GRE A\n",
      "HOL A\n",
      "ING A\n",
      "ING B\n",
      "ING C\n",
      "ING D\n",
      "ITA A\n",
      "ITA B\n",
      "POR A\n",
      "SUE A\n",
      "TUR A\n"
     ]
    }
   ],
   "source": [
    "# Imprime as chaves do dicionário, que são os nomes dos modelos carregados\n",
    "print(\"Modelos carregados:\")\n",
    "for key in loaded_models_lasso.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando as variâncias e as médias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance for ALE A Lasso (team1): 8.257657908034806\n",
      "Mean of y1 for ALE ALasso (team1):5.514530892448513\n",
      "Variance for ALE A Lasso (team2): 6.484798767831093\n",
      "Mean of y2 for ALE ALasso (team1):4.424141876430206\n",
      "--------------------------------------------------\n",
      "Variance for ALE B Lasso (team1): 8.242468053910818\n",
      "Mean of y1 for ALE BLasso (team1):5.514530892448513\n",
      "Variance for ALE B Lasso (team2): 6.472747008042849\n",
      "Mean of y2 for ALE BLasso (team1):4.424141876430206\n",
      "--------------------------------------------------\n",
      "Variance for BEL A Lasso (team1): 8.044445360939637\n",
      "Mean of y1 for BEL ALasso (team1):5.4787757817697935\n",
      "Variance for BEL A Lasso (team2): 6.236534616414278\n",
      "Mean of y2 for BEL ALasso (team1):4.402395209580838\n",
      "--------------------------------------------------\n",
      "Variance for BRA A Lasso (team1): 8.323908580355768\n",
      "Mean of y1 for BRA ALasso (team1):5.841324145906616\n",
      "Variance for BRA A Lasso (team2): 6.969015238588984\n",
      "Mean of y2 for BRA ALasso (team1):4.859056521948634\n",
      "--------------------------------------------------\n",
      "Variance for ESC A Lasso (team1): 8.34200033271317\n",
      "Mean of y1 for ESC ALasso (team1):5.851703669694684\n",
      "Variance for ESC A Lasso (team2): 6.912917064447769\n",
      "Mean of y2 for ESC ALasso (team1):4.845079198504575\n",
      "--------------------------------------------------\n",
      "Variance for ESP A Lasso (team1): 8.447214578423338\n",
      "Mean of y1 for ESP ALasso (team1):5.765669014084507\n",
      "Variance for ESP A Lasso (team2): 6.463638701359867\n",
      "Mean of y2 for ESP ALasso (team1):4.445422535211268\n",
      "--------------------------------------------------\n",
      "Variance for ESP B Lasso (team1): 7.4893450023098795\n",
      "Mean of y1 for ESP BLasso (team1):5.340059888035412\n",
      "Variance for ESP B Lasso (team2): 5.884670514215958\n",
      "Mean of y2 for ESP BLasso (team1):4.193204009894545\n",
      "--------------------------------------------------\n",
      "Variance for FRA A Lasso (team1): 8.218704362558785\n",
      "Mean of y1 for FRA ALasso (team1):5.6303834476554595\n",
      "Variance for FRA A Lasso (team2): 6.394953409993948\n",
      "Mean of y2 for FRA ALasso (team1):4.405993478016417\n",
      "--------------------------------------------------\n",
      "Variance for FRA B Lasso (team1): 7.420554820812204\n",
      "Mean of y1 for FRA BLasso (team1):5.218686131386861\n",
      "Variance for FRA B Lasso (team2): 5.815824832955963\n",
      "Mean of y2 for FRA BLasso (team1):4.047007299270073\n",
      "--------------------------------------------------\n",
      "Variance for GRE A Lasso (team1): 7.1926707440613535\n",
      "Mean of y1 for GRE ALasso (team1):5.033333333333333\n",
      "Variance for GRE A Lasso (team2): 5.571337407961292\n",
      "Mean of y2 for GRE ALasso (team1):3.9537414965986395\n",
      "--------------------------------------------------\n",
      "Variance for HOL A Lasso (team1): 8.049555494519636\n",
      "Mean of y1 for HOL ALasso (team1):5.484489051094891\n",
      "Variance for HOL A Lasso (team2): 6.2224671399621165\n",
      "Mean of y2 for HOL ALasso (team1):4.410948905109489\n",
      "--------------------------------------------------\n",
      "Variance for ING A Lasso (team1): 8.611542705008581\n",
      "Mean of y1 for ING ALasso (team1):5.919142701376584\n",
      "Variance for ING A Lasso (team2): 6.9018234974564745\n",
      "Mean of y2 for ING ALasso (team1):4.7828131388851025\n",
      "--------------------------------------------------\n",
      "Variance for ING B Lasso (team1): 8.566126565414097\n",
      "Mean of y1 for ING BLasso (team1):5.866218621206444\n",
      "Variance for ING B Lasso (team2): 6.977939802606193\n",
      "Mean of y2 for ING BLasso (team1):4.796435931060322\n",
      "--------------------------------------------------\n",
      "Variance for ING C Lasso (team1): 8.525001618970451\n",
      "Mean of y1 for ING CLasso (team1):5.890102740654395\n",
      "Variance for ING C Lasso (team2): 6.993283703315012\n",
      "Mean of y2 for ING CLasso (team1):4.834611943939928\n",
      "--------------------------------------------------\n",
      "Variance for ING D Lasso (team1): 8.333682907924633\n",
      "Mean of y1 for ING DLasso (team1):5.851703669694684\n",
      "Variance for ING D Lasso (team2): 6.901656018437118\n",
      "Mean of y2 for ING DLasso (team1):4.845079198504575\n",
      "--------------------------------------------------\n",
      "Variance for ITA A Lasso (team1): 8.681545645684455\n",
      "Mean of y1 for ITA ALasso (team1):5.858458289334742\n",
      "Variance for ITA A Lasso (team2): 6.922556719906879\n",
      "Mean of y2 for ITA ALasso (team1):4.727645195353749\n",
      "--------------------------------------------------\n",
      "Variance for ITA B Lasso (team1): 7.718179239405954\n",
      "Mean of y1 for ITA BLasso (team1):5.415929203539823\n",
      "Variance for ITA B Lasso (team2): 6.373780314845147\n",
      "Mean of y2 for ITA BLasso (team1):4.412155127537741\n",
      "--------------------------------------------------\n",
      "Variance for POR A Lasso (team1): 8.383354518366783\n",
      "Mean of y1 for POR ALasso (team1):5.81107827096638\n",
      "Variance for POR A Lasso (team2): 6.893863725596579\n",
      "Mean of y2 for POR ALasso (team1):4.790230643373621\n",
      "--------------------------------------------------\n",
      "Variance for SUE A Lasso (team1): 8.307793426071031\n",
      "Mean of y1 for SUE ALasso (team1):5.837275535817725\n",
      "Variance for SUE A Lasso (team2): 6.966309999126063\n",
      "Mean of y2 for SUE ALasso (team1):4.858611701100599\n",
      "--------------------------------------------------\n",
      "Variance for TUR A Lasso (team1): 8.086331915855347\n",
      "Mean of y1 for TUR ALasso (team1):5.473429951690822\n",
      "Variance for TUR A Lasso (team2): 6.097275591921745\n",
      "Mean of y2 for TUR ALasso (team1):4.371497584541062\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Função para calcular a variância das previsões\n",
    "def calculate_variance(model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    #print(type(y_pred), y_pred.shape)\n",
    "    #print(type(y_true), y_true.shape)\n",
    "\n",
    "    variance = np.var(y_pred - y_true.values.flatten())\n",
    "    return variance\n",
    "\n",
    "# Dicionário para armazenar as variâncias dos modelos\n",
    "variances_lasso = {}\n",
    "\n",
    "# Loop para calcular a variância para cada modelo\n",
    "for key in dfs_X.keys():\n",
    "    if key in loaded_models_lasso:\n",
    "        # Recuperar os modelos Lasso carregados para essa chave\n",
    "        model1_lasso = loaded_models_lasso[key].get('model1')\n",
    "        model2_lasso = loaded_models_lasso[key].get('model2')\n",
    "\n",
    "        # Se os modelos existem, calcula a variância\n",
    "        if model1_lasso and model2_lasso:\n",
    "            variance1 = calculate_variance(model1_lasso, dfs_X[key], dfs_y1[key])\n",
    "            variance2 = calculate_variance(model2_lasso, dfs_X[key], dfs_y2[key])\n",
    "\n",
    "            # Armazenar as variâncias calculadas\n",
    "            variances_lasso[key] = {\"variance1\": variance1, \"variance2\": variance2}\n",
    "\n",
    "            # Imprimir as variâncias\n",
    "            print(f\"Variance for {key} Lasso (team1): {variance1}\")\n",
    "            print(f\"Mean of y1 for {key}Lasso (team1):{mean_y1[key]}\")\n",
    "            print(f\"Variance for {key} Lasso (team2): {variance2}\")\n",
    "            print(f\"Mean of y2 for {key}Lasso (team1):{mean_y2[key]}\")\n",
    "            print('-'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o X e o Ys corretamente para a entrada na rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359056, 1)\n",
      "(359056,)\n",
      "(359056,)\n"
     ]
    }
   ],
   "source": [
    "# Criar listas vazias para armazenar as previsões e os labels\n",
    "all_lasso_predictions = []\n",
    "all_y1_labels = []\n",
    "all_y2_labels = []\n",
    "\n",
    "# Iterar por cada campeonato\n",
    "for championship, X in dfs_X.items():\n",
    "    # Use o modelo Lasso correspondente para fazer as previsões\n",
    "    model1_lasso_loaded = loaded_models_lasso[championship][\"model1\"]\n",
    "    model2_lasso_loaded = loaded_models_lasso[championship][\"model2\"]\n",
    "    \n",
    "    # Fazendo as previsões\n",
    "    preds = model1_lasso_loaded.predict(X)\n",
    "    \n",
    "    # Concatenando as previsões e os labels\n",
    "    all_lasso_predictions.extend(preds)\n",
    "    all_y1_labels.extend(dfs_y1[championship]['team1_corners'])\n",
    "    all_y2_labels.extend(dfs_y2[championship]['team2_corners'])\n",
    "\n",
    "# Converter listas em arrays numpy para treinamento de rede neural\n",
    "all_lasso_predictions = np.array(all_lasso_predictions).reshape(-1, 1)  # O reshape é para torná-lo uma matriz de uma única coluna\n",
    "all_y1_labels = np.array(all_y1_labels)\n",
    "all_y2_labels = np.array(all_y2_labels)\n",
    "\n",
    "# Agora, você pode usar all_lasso_predictions como entrada e all_y_labels como saída para treinar sua rede neural.\n",
    "print(all_lasso_predictions.shape)\n",
    "print(all_y1_labels.shape)\n",
    "print(all_y2_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando as ocorrências dos dados para tirar insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de all_y1_labels: 5.777900940243304\n",
      "Valores Únicos para all_y1_labels:\n",
      "Valor: 0.0, Ocorrências: 3523, Porcentagem: 0.98%\n",
      "Valor: 1.0, Ocorrências: 12856, Porcentagem: 3.58%\n",
      "Valor: 2.0, Ocorrências: 25984, Porcentagem: 7.24%\n",
      "Valor: 3.0, Ocorrências: 39878, Porcentagem: 11.11%\n",
      "Valor: 4.0, Ocorrências: 48836, Porcentagem: 13.60%\n",
      "Valor: 5.0, Ocorrências: 50853, Porcentagem: 14.16%\n",
      "Valor: 6.0, Ocorrências: 46542, Porcentagem: 12.96%\n",
      "Valor: 7.0, Ocorrências: 39230, Porcentagem: 10.93%\n",
      "Valor: 8.0, Ocorrências: 30337, Porcentagem: 8.45%\n",
      "Valor: 9.0, Ocorrências: 21399, Porcentagem: 5.96%\n",
      "Valor: 10.0, Ocorrências: 15118, Porcentagem: 4.21%\n",
      "Valor: 11.0, Ocorrências: 9802, Porcentagem: 2.73%\n",
      "Valor: 12.0, Ocorrências: 6037, Porcentagem: 1.68%\n",
      "Valor: 13.0, Ocorrências: 3835, Porcentagem: 1.07%\n",
      "Valor: 14.0, Ocorrências: 2317, Porcentagem: 0.65%\n",
      "Valor: 15.0, Ocorrências: 2509, Porcentagem: 0.70%\n",
      "\n",
      "Média de all_y2_labels: 4.707744752907624\n",
      "Valores Únicos para all_y2_labels:\n",
      "Valor: 0.0, Ocorrências: 7570, Porcentagem: 2.11%\n",
      "Valor: 1.0, Ocorrências: 24954, Porcentagem: 6.95%\n",
      "Valor: 2.0, Ocorrências: 41998, Porcentagem: 11.70%\n",
      "Valor: 3.0, Ocorrências: 54193, Porcentagem: 15.09%\n",
      "Valor: 4.0, Ocorrências: 57015, Porcentagem: 15.88%\n",
      "Valor: 5.0, Ocorrências: 51221, Porcentagem: 14.27%\n",
      "Valor: 6.0, Ocorrências: 40265, Porcentagem: 11.21%\n",
      "Valor: 7.0, Ocorrências: 31039, Porcentagem: 8.64%\n",
      "Valor: 8.0, Ocorrências: 19857, Porcentagem: 5.53%\n",
      "Valor: 9.0, Ocorrências: 13069, Porcentagem: 3.64%\n",
      "Valor: 10.0, Ocorrências: 7676, Porcentagem: 2.14%\n",
      "Valor: 11.0, Ocorrências: 4832, Porcentagem: 1.35%\n",
      "Valor: 12.0, Ocorrências: 2379, Porcentagem: 0.66%\n",
      "Valor: 13.0, Ocorrências: 1584, Porcentagem: 0.44%\n",
      "Valor: 14.0, Ocorrências: 802, Porcentagem: 0.22%\n",
      "Valor: 15.0, Ocorrências: 602, Porcentagem: 0.17%\n"
     ]
    }
   ],
   "source": [
    "# Para all_y1_labels\n",
    "total_count_y1 = len(all_y1_labels)\n",
    "unique_values_y1, counts_y1 = np.unique(all_y1_labels, return_counts=True)\n",
    "mean_y1 = np.mean(all_y1_labels)\n",
    "\n",
    "print(f\"Média de all_y1_labels: {mean_y1}\")\n",
    "print(\"Valores Únicos para all_y1_labels:\")\n",
    "for value, count in zip(unique_values_y1, counts_y1):\n",
    "    percentage = (count / total_count_y1) * 100\n",
    "    print(f\"Valor: {value}, Ocorrências: {count}, Porcentagem: {percentage:.2f}%\")\n",
    "\n",
    "# Para all_y2_labels\n",
    "total_count_y2 = len(all_y2_labels)\n",
    "unique_values_y2, counts_y2 = np.unique(all_y2_labels, return_counts=True)\n",
    "mean_y2 = np.mean(all_y2_labels)\n",
    "\n",
    "print(f\"\\nMédia de all_y2_labels: {mean_y2}\")\n",
    "print(\"Valores Únicos para all_y2_labels:\")\n",
    "for value, count in zip(unique_values_y2, counts_y2):\n",
    "    percentage = (count / total_count_y2) * 100\n",
    "    print(f\"Valor: {value}, Ocorrências: {count}, Porcentagem: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values and counts for all_lasso_predictions:\n",
      "Values: [2.67 2.88 2.95 2.98 3.03 3.08 3.12 3.17 3.19 3.22 3.23 3.25 3.29 3.3\n",
      " 3.35 3.36 3.38 3.41 3.42 3.43 3.45 3.5  3.51 3.53 3.56 3.58 3.59 3.61\n",
      " 3.62 3.63 3.64 3.67 3.68 3.71 3.73 3.74 3.75 3.77 3.78 3.79 3.8  3.81\n",
      " 3.82 3.83 3.84 3.85 3.87 3.88 3.89 3.91 3.92 3.93 3.94 3.95 3.96 3.97\n",
      " 3.98 3.99 4.   4.01 4.02 4.03 4.04 4.05 4.06 4.07 4.08 4.09 4.1  4.11\n",
      " 4.12 4.13 4.14 4.15 4.16 4.17 4.18 4.19 4.2  4.21 4.22 4.23 4.24 4.25\n",
      " 4.26 4.27 4.28 4.29 4.3  4.31 4.32 4.33 4.34 4.35 4.36 4.37 4.38 4.39\n",
      " 4.4  4.41 4.42 4.43 4.44 4.45 4.46 4.47 4.48 4.49 4.5  4.51 4.52 4.53\n",
      " 4.54 4.55 4.56 4.57 4.58 4.59 4.6  4.61 4.62 4.63 4.64 4.65 4.66 4.67\n",
      " 4.68 4.69 4.7  4.71 4.72 4.73 4.74 4.75 4.76 4.77 4.78 4.79 4.8  4.81\n",
      " 4.82 4.83 4.84 4.85 4.86 4.87 4.88 4.89 4.9  4.91 4.92 4.93 4.94 4.95\n",
      " 4.96 4.97 4.98 4.99 5.   5.01 5.02 5.03 5.04 5.05 5.06 5.07 5.08 5.09\n",
      " 5.1  5.11 5.12 5.13 5.14 5.15 5.16 5.17 5.18 5.19 5.2  5.21 5.22 5.23\n",
      " 5.24 5.25 5.26 5.27 5.28 5.29 5.3  5.31 5.32 5.33 5.34 5.35 5.36 5.37\n",
      " 5.38 5.39 5.4  5.41 5.42 5.43 5.44 5.45 5.46 5.47 5.48 5.49 5.5  5.51\n",
      " 5.52 5.53 5.54 5.55 5.56 5.57 5.58 5.59 5.6  5.61 5.62 5.63 5.64 5.65\n",
      " 5.66 5.67 5.68 5.69 5.7  5.71 5.72 5.73 5.74 5.75 5.76 5.77 5.78 5.79\n",
      " 5.8  5.81 5.82 5.83 5.84 5.85 5.86 5.87 5.88 5.89 5.9  5.91 5.92 5.93\n",
      " 5.94 5.95 5.96 5.97 5.98 5.99 6.   6.01 6.02 6.03 6.04 6.05 6.06 6.07\n",
      " 6.08 6.09 6.1  6.11 6.12 6.13 6.14 6.15 6.16 6.17 6.18 6.19 6.2  6.21\n",
      " 6.22 6.23 6.24 6.25 6.26 6.27 6.28 6.29 6.3  6.31 6.32 6.33 6.34 6.35\n",
      " 6.36 6.37 6.38 6.39 6.4  6.41 6.42 6.43 6.44 6.45 6.46 6.47 6.48 6.49\n",
      " 6.5  6.51 6.52 6.53 6.54 6.55 6.56 6.57 6.58 6.59 6.6  6.61 6.62 6.63\n",
      " 6.64 6.65 6.66 6.67 6.68 6.69 6.7  6.71 6.72 6.73 6.74 6.75 6.76 6.77\n",
      " 6.78 6.79 6.8  6.81 6.82 6.83 6.84 6.85 6.86 6.87 6.88 6.89 6.9  6.91\n",
      " 6.92 6.93 6.94 6.95 6.96 6.97 6.98 6.99 7.   7.01 7.03 7.04 7.05 7.06\n",
      " 7.07 7.09 7.1  7.13 7.14 7.15 7.17 7.19 7.2  7.21 7.24 7.28 7.29 7.31\n",
      " 7.32 7.34 7.35 7.36 7.42 7.46 7.53]\n",
      "Counts: [   1    1    1    1    1    1    1    1    2    1    1    1    2    1\n",
      "    1    2    1    1    3    1    2    1    1    2    1    3    3    1\n",
      "    1    1    2    4    1    2    2    1    3    7    2    3    2    2\n",
      "    5    3    3    6    5    3    2    3    2    2    7    3    3    7\n",
      "    4    1    3    5    7    5    3    6    8    3    2    4   11    5\n",
      "   10    4   10    7    9    9   10    8    8    8   10   10   16   13\n",
      "   16   15   19   10    9    9   14   22   18   19   24   19   21   20\n",
      "   23   27   31   38   26   27   35   28   29   33   40   42   37   39\n",
      "   56   43   47   58   52   61   55   58   76   74   77   73   67   79\n",
      "   80   97   74  101  112  110  119  106  146  107  133  122  162  159\n",
      "  170  152  187  188  198  205  195  212  233  214  254  257  253  282\n",
      "  275  264  309  338  341  340  383  384  356  363  414  400  452  437\n",
      "  465  480  535  517  551  586  628  656  633  675  677  707  747  756\n",
      "  819  840  810  919  902  967  991 1049 1081 1203 1222 1275 1284 1334\n",
      " 1475 1468 1474 1605 1772 1708 1865 1933 2030 2082 2181 2371 2426 2512\n",
      " 2596 2603 2870 2992 3036 3172 3209 3325 3410 3499 3571 3854 3840 3882\n",
      " 3972 4183 4208 4278 4349 4463 4495 4586 4632 4761 4751 4860 4758 4795\n",
      " 4912 4876 4959 4893 5013 4894 4956 4895 4804 4920 4882 5014 4735 4725\n",
      " 4836 4591 4454 4549 4439 4352 4292 4116 3909 3905 3790 3914 3550 3392\n",
      " 3504 3369 3204 2963 2936 2772 2694 2586 2468 2271 2221 2045 1999 1901\n",
      " 1716 1575 1572 1440 1293 1281 1195 1107 1032  984  868  873  747  662\n",
      "  704  616  571  519  433  432  407  389  333  329  315  309  235  226\n",
      "  225  195  166  164  170  137  116  117  111  105   87   90   71   69\n",
      "   65   47   54   50   31   48   37   36   34   25   25   28   26   23\n",
      "   24   27   20   15    9   19   18   11   12   14   10    4   10    9\n",
      "    9    6    9    7    7    4    4    5    4    3    4    2    6    1\n",
      "    4    2    1    2    4    1    1    1    1    1    2    2    1    1\n",
      "    1    1    1    2    1    1    1]\n",
      "\n",
      "Unique values and counts for all_y1_labels:\n",
      "Values: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15.]\n",
      "Counts: [ 3523 12856 25984 39878 48836 50853 46542 39230 30337 21399 15118  9802\n",
      "  6037  3835  2317  2509]\n",
      "\n",
      "Unique values and counts for all_y2_labels:\n",
      "Values: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15.]\n",
      "Counts: [ 7570 24954 41998 54193 57015 51221 40265 31039 19857 13069  7676  4832\n",
      "  2379  1584   802   602]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Supondo que all_lasso_predictions, all_y1_labels e all_y2_labels são seus arrays NumPy\n",
    "\n",
    "# Para valores inteiros\n",
    "unique_values_lasso, counts_lasso = np.unique(all_lasso_predictions, return_counts=True)\n",
    "unique_values_y1, counts_y1 = np.unique(all_y1_labels, return_counts=True)\n",
    "unique_values_y2, counts_y2 = np.unique(all_y2_labels, return_counts=True)\n",
    "\n",
    "# Para valores de ponto flutuante, você pode arredondar até um determinado número de casas decimais, digamos 2\n",
    "unique_values_lasso, counts_lasso = np.unique(np.round(all_lasso_predictions, 2), return_counts=True)\n",
    "unique_values_y1, counts_y1 = np.unique(np.round(all_y1_labels, 2), return_counts=True)\n",
    "unique_values_y2, counts_y2 = np.unique(np.round(all_y2_labels, 2), return_counts=True)\n",
    "\n",
    "# Imprimir os valores únicos e suas contagens\n",
    "print(\"Unique values and counts for all_lasso_predictions:\")\n",
    "print(\"Values:\", unique_values_lasso)\n",
    "print(\"Counts:\", counts_lasso)\n",
    "\n",
    "print(\"\\nUnique values and counts for all_y1_labels:\")\n",
    "print(\"Values:\", unique_values_y1)\n",
    "print(\"Counts:\", counts_y1)\n",
    "\n",
    "print(\"\\nUnique values and counts for all_y2_labels:\")\n",
    "print(\"Values:\", unique_values_y2)\n",
    "print(\"Counts:\", counts_y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando uma rede neural para fazer calcular as probabilidades das output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1\n",
      "Epoch 1/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.6233 - accuracy: 0.1305 - val_loss: 2.4881 - val_accuracy: 0.1279\n",
      "Epoch 2/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4583 - accuracy: 0.1385 - val_loss: 2.4499 - val_accuracy: 0.1426\n",
      "Epoch 3/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4471 - accuracy: 0.1412 - val_loss: 2.4478 - val_accuracy: 0.1426\n",
      "Epoch 4/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4476 - val_accuracy: 0.1426\n",
      "Epoch 5/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 6/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 7/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 8/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4476 - val_accuracy: 0.1426\n",
      "Epoch 9/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 10/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 11/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 12/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 13/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 14/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 15/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4476 - val_accuracy: 0.1426\n",
      "Epoch 16/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1409 - val_loss: 2.4474 - val_accuracy: 0.1426\n",
      "Epoch 17/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 18/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 19/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4476 - val_accuracy: 0.1426\n",
      "Epoch 20/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4474 - val_accuracy: 0.1426\n",
      "Epoch 21/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 22/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1410 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 23/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 24/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 25/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4476 - val_accuracy: 0.1363\n",
      "Epoch 26/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1409 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 27/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4474 - val_accuracy: 0.1426\n",
      "Epoch 28/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4476 - val_accuracy: 0.1426\n",
      "Epoch 29/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4475 - val_accuracy: 0.1426\n",
      "Epoch 30/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4463 - accuracy: 0.1412 - val_loss: 2.4474 - val_accuracy: 0.1426\n",
      "Model2\n",
      "Epoch 1/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.6259 - accuracy: 0.1557 - val_loss: 2.5157 - val_accuracy: 0.1592\n",
      "Epoch 2/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.4563 - accuracy: 0.1586 - val_loss: 2.4132 - val_accuracy: 0.1592\n",
      "Epoch 3/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3895 - accuracy: 0.1586 - val_loss: 2.3722 - val_accuracy: 0.1592\n",
      "Epoch 4/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3595 - accuracy: 0.1585 - val_loss: 2.3336 - val_accuracy: 0.1592\n",
      "Epoch 5/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3329 - accuracy: 0.1579 - val_loss: 2.3332 - val_accuracy: 0.1592\n",
      "Epoch 6/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3327 - accuracy: 0.1584 - val_loss: 2.3334 - val_accuracy: 0.1506\n",
      "Epoch 7/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3327 - accuracy: 0.1582 - val_loss: 2.3332 - val_accuracy: 0.1592\n",
      "Epoch 8/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3327 - accuracy: 0.1583 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 9/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1585 - val_loss: 2.3331 - val_accuracy: 0.1592\n",
      "Epoch 10/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3327 - accuracy: 0.1584 - val_loss: 2.3332 - val_accuracy: 0.1506\n",
      "Epoch 11/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1584 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 12/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1584 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 13/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1582 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 14/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1582 - val_loss: 2.3331 - val_accuracy: 0.1592\n",
      "Epoch 15/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1586 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 16/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1585 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 17/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1584 - val_loss: 2.3331 - val_accuracy: 0.1592\n",
      "Epoch 18/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 19/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1586 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 20/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3331 - val_accuracy: 0.1592\n",
      "Epoch 21/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3326 - accuracy: 0.1586 - val_loss: 2.3331 - val_accuracy: 0.1592\n",
      "Epoch 22/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 23/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 24/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1581 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 25/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 26/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 27/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 28/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 29/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 30/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 31/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3330 - val_accuracy: 0.1592\n",
      "Epoch 32/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1584 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 33/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 34/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 35/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1583 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 36/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 37/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 38/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3324 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n",
      "Epoch 39/100\n",
      "491/491 [==============================] - 1s 2ms/step - loss: 2.3325 - accuracy: 0.1586 - val_loss: 2.3329 - val_accuracy: 0.1592\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Carregar os dados\n",
    "X = np.array(all_lasso_predictions)  # Valores previstos pelo modelo Lasso\n",
    "y1 = np.array(all_y1_labels)  # Valores reais de escanteios\n",
    "y2 = np.array(all_y2_labels)  # Valores reais de escanteios\n",
    "\n",
    "# Converter y para one-hot encoding\n",
    "y1_onehot = to_categorical(y1, num_classes=16)\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(X, y1_onehot, test_size=0.3, random_state=42)\n",
    "\n",
    "# Converter y para one-hot encoding\n",
    "y2_onehot = to_categorical(y2, num_classes=16)\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y2_train, y2_test = train_test_split(X, y2_onehot, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Construindo o modelo\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(1, input_dim=1, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)))  # Regularização L1 e L2\n",
    "#model1.add(Dropout(0.2))  # Dropout para regularização\n",
    "model1.add(Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)))  # Regularização L1 e L2\n",
    "model1.add(Dense(16, activation='softmax'))\n",
    "\n",
    "# Compilar o modelo\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callback para Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Treinar o modelo e armazenar histórico do treinamento na variável 'history'\n",
    "print(\"Model1\")\n",
    "history1 = model1.fit(X_train, y1_train, epochs=100, batch_size=512, validation_data=(X_test, y1_test), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Construindo o modelo\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(1, input_dim=1, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)))  # Regularização L1 e L2\n",
    "#model2.add(Dropout(0.2))  # Dropout para regularização\n",
    "model2.add(Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)))  # Regularização L1 e L2\n",
    "model2.add(Dense(16, activation='softmax'))\n",
    "\n",
    "# Compilar o modelo\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callback para Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "print(\"Model2\")\n",
    "history2 = model2.fit(X_train, y2_train, epochs=100, batch_size=512, validation_data=(X_test, y2_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando as previsões DIRETO DO MODELO LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team 1 (flamengo) Lasso Prediction: 6.109434717564853\n",
      "Team 2 (internacional) Lasso Prediction: 5.000847899364216\n",
      "----\n",
      "Team 1 (corinthians) Lasso Prediction: 5.930629308872838\n",
      "Team 2 (goiás) Lasso Prediction: 4.942319737042925\n",
      "----\n",
      "Team 1 (red bull bragantino) Lasso Prediction: 6.1564280448159625\n",
      "Team 2 (cuiabá) Lasso Prediction: 4.916069542417268\n",
      "----\n",
      "Team 1 (atlético mineiro) Lasso Prediction: 6.157234109805144\n",
      "Team 2 (santos) Lasso Prediction: 4.966723638017235\n",
      "----\n",
      "Team 1 (palmeiras) Lasso Prediction: 5.710385978004586\n",
      "Team 2 (vasco da gama) Lasso Prediction: 4.776241097155892\n",
      "----\n",
      "Team 1 (botafogo) Lasso Prediction: 6.095631220923581\n",
      "Team 2 (bahia) Lasso Prediction: 4.70892473952414\n",
      "----\n",
      "Team 1 (américa-mg) Lasso Prediction: 6.2433332229498335\n",
      "Team 2 (são paulo) Lasso Prediction: 4.721866121895038\n",
      "----\n",
      "Team 1 (athletico-pr) Lasso Prediction: 5.962934742553537\n",
      "Team 2 (fluminense) Lasso Prediction: 4.584950487155758\n",
      "----\n",
      "Team 1 (grêmio) Lasso Prediction: 6.000915450486873\n",
      "Team 2 (cruzeiro) Lasso Prediction: 5.2370702973587475\n",
      "----\n",
      "Team 1 (fortaleza) Lasso Prediction: 6.048631914978567\n",
      "Team 2 (coritiba) Lasso Prediction: 4.79344314488727\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for _, row in future_matches_filtred_scaled.iterrows():\n",
    "    # Descobrir qual modelo usar\n",
    "    championship = row['championship'].upper()\n",
    "    \n",
    "    # Buscar os modelos apropriados carregados\n",
    "    model1_lasso_loaded = loaded_models_lasso[championship][\"model1\"]\n",
    "    model2_lasso_loaded = loaded_models_lasso[championship][\"model2\"]\n",
    "    \n",
    "    # Obter os nomes das equipes antes de usar o output.append(...)\n",
    "    team1_name = le.inverse_transform([int(row['team1'])])[0]\n",
    "    team2_name = le.inverse_transform([int(row['team2'])])[0]\n",
    "    \n",
    "    # Remover a coluna 'championship' após ter selecionado o modelo\n",
    "    row_dropped = row.drop('championship')\n",
    "    \n",
    "    # Fazer previsões usando os modelos Lasso carregados\n",
    "    team1_lasso_prediction = model1_lasso_loaded.predict([row_dropped])\n",
    "    team2_lasso_prediction = model2_lasso_loaded.predict([row_dropped])\n",
    "    \n",
    "    # Imprimir as previsões\n",
    "    print(f\"Team 1 ({team1_name}) Lasso Prediction: {team1_lasso_prediction[0]}\")\n",
    "    print(f\"Team 2 ({team2_name}) Lasso Prediction: {team2_lasso_prediction[0]}\")\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output usando NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def nn_probabilities(input_data, model):\n",
    "    \"\"\"\n",
    "    Calcula as probabilidades usando o modelo neural passado como argumento.\n",
    "    \"\"\"\n",
    "    input_data = np.array([input_data]).reshape(-1, 1)\n",
    "    probabilities = model.predict(input_data)\n",
    "    return probabilities[0]\n",
    "\n",
    "def calculate_probabilities(match_row, model1_lasso, model2_lasso, model1_nn, model2_nn):\n",
    "    input_data = match_row.copy()\n",
    "    \n",
    "    team1_goal_prediction = model1_lasso.predict([input_data])\n",
    "    team2_goal_prediction = model2_lasso.predict([input_data])\n",
    "    \n",
    "    team1_goal_probabilities = nn_probabilities(team1_goal_prediction[0], model1_nn)\n",
    "    team2_goal_probabilities = nn_probabilities(team2_goal_prediction[0], model2_nn)\n",
    "    \n",
    "\n",
    "    joint_prob_matrix = np.outer(team1_goal_probabilities, team2_goal_probabilities)\n",
    "\n",
    "    team1_win_prob = np.sum(np.tril(joint_prob_matrix, -1))\n",
    "    draw_prob = np.sum(np.diag(joint_prob_matrix))\n",
    "    team2_win_prob = np.sum(np.triu(joint_prob_matrix, 1))\n",
    "\n",
    "    team1_minus25_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if i-j>=3])\n",
    "    team1_plus25_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if i-j>=-2])\n",
    "    team2_minus25_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if j-i>=3])\n",
    "    team2_plus25_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if j-i>=-2])\n",
    "\n",
    "    team1_minus15_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if i-j>=2])\n",
    "    team1_plus15_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if i-j>=-1])\n",
    "    team2_minus15_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if j-i>=2])\n",
    "    team2_plus15_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if j-i>=-1])\n",
    "\n",
    "    team1_over45_prob = 1 - sum(team1_goal_probabilities[:5])\n",
    "    team1_under45_prob = sum(team1_goal_probabilities[:5])\n",
    "    team2_over45_prob = 1 - sum(team2_goal_probabilities[:5])\n",
    "    team2_under45_prob = sum(team2_goal_probabilities[:5])\n",
    "\n",
    "    team1_over55_prob = 1 - sum(team1_goal_probabilities[:6])\n",
    "    team1_under55_prob = sum(team1_goal_probabilities[:6])\n",
    "    team2_over55_prob = 1 - sum(team2_goal_probabilities[:6])\n",
    "    team2_under55_prob = sum(team2_goal_probabilities[:6])\n",
    "    \n",
    "    return (team1_win_prob, draw_prob, team2_win_prob,\n",
    "        team1_minus15_prob, team1_plus15_prob, team2_minus15_prob, team2_plus15_prob,\n",
    "        team1_minus25_prob, team1_plus25_prob, team2_minus25_prob, team2_plus25_prob,\n",
    "        team1_over45_prob, team1_under45_prob, team2_over45_prob, team2_under45_prob,\n",
    "        team1_over55_prob, team1_under55_prob, team2_over55_prob, team2_under55_prob)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "for _, row in future_matches_filtred_scaled.iterrows():\n",
    "    # Descobrir qual modelo usar\n",
    "    championship = row['championship'].upper()\n",
    "    \n",
    "    # Buscar os modelos apropriados carregados\n",
    "    model1_lasso_loaded = loaded_models_lasso[championship][\"model1\"]\n",
    "    model2_lasso_loaded = loaded_models_lasso[championship][\"model2\"]\n",
    "    \n",
    "    # Obter os nomes das equipes antes de usar o output.append(...)\n",
    "    team1_name = le.inverse_transform([int(row['team1'])])[0]\n",
    "    team2_name = le.inverse_transform([int(row['team2'])])[0]\n",
    "    \n",
    "    # Remover a coluna 'championship' após ter selecionado o modelo\n",
    "    row = row.drop('championship')\n",
    "    \n",
    "    # Calcular probabilidades\n",
    "    (team1_win_prob, draw_prob, team2_win_prob,\n",
    "     team1_minus15_prob, team1_plus15_prob, team2_minus15_prob, team2_plus15_prob,\n",
    "     team1_minus25_prob, team1_plus25_prob, team2_minus25_prob, team2_plus25_prob,\n",
    "     team1_over45_prob, team1_under45_prob, team2_over45_prob, team2_under45_prob,\n",
    "     team1_over55_prob, team1_under55_prob, team2_over55_prob, team2_under55_prob) = calculate_probabilities(row, model1_lasso_loaded, model2_lasso_loaded, model1, model2)\n",
    "    \n",
    "    output.append([team1_name, team2_name, 1/team1_win_prob, 1/draw_prob, 1/team2_win_prob,\n",
    "                   1/team1_minus15_prob, 1/team1_plus15_prob, 1/team2_minus15_prob, 1/team2_plus15_prob,\n",
    "                   1/team1_minus25_prob, 1/team1_plus25_prob, 1/team2_minus25_prob, 1/team2_plus25_prob,\n",
    "                   1/team1_over45_prob, 1/team1_under45_prob, 1/team2_over45_prob, 1/team2_under45_prob,\n",
    "                   1/team1_over55_prob, 1/team1_under55_prob, 1/team2_over55_prob, 1/team2_under55_prob])\n",
    "    \n",
    "\n",
    "df_output = pd.DataFrame()\n",
    "\n",
    "df_output['date'] = pd.to_datetime(future_matches['date'].values, format='%d/%m/%Y').date\n",
    "df_output['championship'] = future_matches['championship'].values\n",
    "\n",
    "columns=['Team 1', 'Team 2', 'Team 1 Win Odd', 'Draw Odd', 'Team 2 Win Odd',\n",
    "         'Team 1 -1.5 Odd', 'Team 1 +1.5 Odd', 'Team 2 -1.5 Odd', 'Team 2 +1.5 Odd',\n",
    "         'Team 1 -2.5 Odd', 'Team 1 +2.5 Odd', 'Team 2 -2.5 Odd', 'Team 2 +2.5 Odd',\n",
    "         'Team 1 Over 4.5', 'Team 1 Under 4.5', 'Team 2 Over 4.5', 'Team 2 Under 4.5',\n",
    "         'Team 1 Over 5.5', 'Team 1 Under 5.5', 'Team 2 Over 5.5', 'Team 2 Under 5.5']\n",
    "\n",
    "\n",
    "df_output = df_output.join(pd.DataFrame(output, columns=columns))\n",
    "\n",
    "df_output = df_output.sort_values(['championship', 'date'])\n",
    "\n",
    "df_output.to_excel(\"output_NN_corners_AH.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output usando Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def poisson_probabilities(lam, max_corners=15):\n",
    "    probs = [poisson.pmf(k, lam) for k in range(max_corners + 1)]\n",
    "    probs.append(1 - sum(probs))\n",
    "    return probs\n",
    "\n",
    "def calculate_probabilities(match_row, model1_lasso, model2_lasso):\n",
    "    input_data = match_row.copy()\n",
    "\n",
    "    team1_goal_prediction = model1_lasso.predict([input_data])\n",
    "    team2_goal_prediction = model2_lasso.predict([input_data])\n",
    "\n",
    "    team1_goal_probabilities = poisson_probabilities(team1_goal_prediction[0])\n",
    "    team2_goal_probabilities = poisson_probabilities(team2_goal_prediction[0])\n",
    "\n",
    "    joint_prob_matrix = np.outer(team1_goal_probabilities, team2_goal_probabilities)\n",
    "\n",
    "    team1_win_prob = np.sum(np.tril(joint_prob_matrix, -1))\n",
    "    draw_prob = np.sum(np.diag(joint_prob_matrix))\n",
    "    team2_win_prob = np.sum(np.triu(joint_prob_matrix, 1))\n",
    "\n",
    "    team1_minus25_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if i-j>=3])\n",
    "    team1_plus25_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if i-j>=-2])\n",
    "    team2_minus25_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if j-i>=3])\n",
    "    team2_plus25_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if j-i>=-2])\n",
    "\n",
    "    team1_minus15_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if i-j>=2])\n",
    "    team1_plus15_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if i-j>=-1])\n",
    "    team2_minus15_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if j-i>=2])\n",
    "    team2_plus15_prob = np.sum([joint_prob_matrix[i,j] for i in range(joint_prob_matrix.shape[0]) for j in range(joint_prob_matrix.shape[1]) if j-i>=-1])\n",
    "\n",
    "    team1_over45_prob = 1 - sum(team1_goal_probabilities[:5])\n",
    "    team1_under45_prob = sum(team1_goal_probabilities[:5])\n",
    "    team2_over45_prob = 1 - sum(team2_goal_probabilities[:5])\n",
    "    team2_under45_prob = sum(team2_goal_probabilities[:5])\n",
    "\n",
    "    team1_over55_prob = 1 - sum(team1_goal_probabilities[:6])\n",
    "    team1_under55_prob = sum(team1_goal_probabilities[:6])\n",
    "    team2_over55_prob = 1 - sum(team2_goal_probabilities[:6])\n",
    "    team2_under55_prob = sum(team2_goal_probabilities[:6])\n",
    "    \n",
    "    return (team1_win_prob, draw_prob, team2_win_prob,\n",
    "        team1_minus15_prob, team1_plus15_prob, team2_minus15_prob, team2_plus15_prob,\n",
    "        team1_minus25_prob, team1_plus25_prob, team2_minus25_prob, team2_plus25_prob,\n",
    "        team1_over45_prob, team1_under45_prob, team2_over45_prob, team2_under45_prob,\n",
    "        team1_over55_prob, team1_under55_prob, team2_over55_prob, team2_under55_prob)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but Lasso was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "for _, row in future_matches_filtred_scaled.iterrows():\n",
    "    # Descobrir qual modelo usar\n",
    "    championship = row['championship'].upper()\n",
    "    \n",
    "    # Buscar os modelos apropriados carregados\n",
    "    model1_lasso_loaded = loaded_models_lasso[championship][\"model1\"]\n",
    "    model2_lasso_loaded = loaded_models_lasso[championship][\"model2\"]\n",
    "    \n",
    "    # Obter os nomes das equipes antes de usar o output.append(...)\n",
    "    team1_name = le.inverse_transform([int(row['team1'])])[0]\n",
    "    team2_name = le.inverse_transform([int(row['team2'])])[0]\n",
    "    \n",
    "    # Remover a coluna 'championship' após ter selecionado o modelo\n",
    "    row = row.drop('championship')\n",
    "    \n",
    "    # Calcular probabilidades\n",
    "    (team1_win_prob, draw_prob, team2_win_prob,\n",
    "     team1_minus15_prob, team1_plus15_prob, team2_minus15_prob, team2_plus15_prob,\n",
    "     team1_minus25_prob, team1_plus25_prob, team2_minus25_prob, team2_plus25_prob,\n",
    "     team1_over45_prob, team1_under45_prob, team2_over45_prob, team2_under45_prob,\n",
    "     team1_over55_prob, team1_under55_prob, team2_over55_prob, team2_under55_prob) = calculate_probabilities(row, model1_lasso_loaded, model2_lasso_loaded)\n",
    "    \n",
    "    output.append([team1_name, team2_name, 1/team1_win_prob, 1/draw_prob, 1/team2_win_prob,\n",
    "                   1/team1_minus15_prob, 1/team1_plus15_prob, 1/team2_minus15_prob, 1/team2_plus15_prob,\n",
    "                   1/team1_minus25_prob, 1/team1_plus25_prob, 1/team2_minus25_prob, 1/team2_plus25_prob,\n",
    "                   1/team1_over45_prob, 1/team1_under45_prob, 1/team2_over45_prob, 1/team2_under45_prob,\n",
    "                   1/team1_over55_prob, 1/team1_under55_prob, 1/team2_over55_prob, 1/team2_under55_prob])\n",
    "    \n",
    "\n",
    "df_output = pd.DataFrame()\n",
    "\n",
    "df_output['date'] = pd.to_datetime(future_matches['date'].values, format='%d/%m/%Y').date\n",
    "df_output['championship'] = future_matches['championship'].values\n",
    "\n",
    "columns=['Team 1', 'Team 2', 'Team 1 Win Odd', 'Draw Odd', 'Team 2 Win Odd',\n",
    "         'Team 1 -1.5 Odd', 'Team 1 +1.5 Odd', 'Team 2 -1.5 Odd', 'Team 2 +1.5 Odd',\n",
    "         'Team 1 -2.5 Odd', 'Team 1 +2.5 Odd', 'Team 2 -2.5 Odd', 'Team 2 +2.5 Odd',\n",
    "         'Team 1 Over 4.5', 'Team 1 Under 4.5', 'Team 2 Over 4.5', 'Team 2 Under 4.5',\n",
    "         'Team 1 Over 5.5', 'Team 1 Under 5.5', 'Team 2 Over 5.5', 'Team 2 Under 5.5']\n",
    "\n",
    "\n",
    "df_output = df_output.join(pd.DataFrame(output, columns=columns))\n",
    "\n",
    "df_output = df_output.sort_values(['championship', 'date'])\n",
    "\n",
    "df_output.to_excel(\"output_corners_AH.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
